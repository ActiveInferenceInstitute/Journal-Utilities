SPEAKER_00:
Thank you, .


SPEAKER_02:
OK, cool.

This has been a really cool morning.

It's really great to find out the parallels between what I plan to talk about and what other people has already talked about.

And nothing has been planned, so this is all emergent.

So yeah, to introduce the

Okay, so I'm Chen Ling.

I'm a PhD candidate at UC Berkeley in the computational biology program, and my research is mainly focused on making tools to analyze single-cell RNA sequencing data.

So a lot of the examples I'm going to give today is related to that research.

and Diamantis Ellis, who is a R&D engineer at Cosmotech.

And he's going to join us via video soon to talk about the evolutionary biology side.

OK.

So biology and complexity science.

So asking me to talk about biology is kind of like asking me to talk about the entire world.

But I think one point that I want to make before we even start is just how biology is relevant to the development of many quantitative science in the past and why I think it's really relevant for developing complexity science in the future.

So this is a quote that Claude Shannon made on information theory right when information theory was getting invented.

So it says, the establishing of such application is not a trivial matter of translating words to a new domain, but rather the slow, tedious process of hypothesis and experimental verification.

So this is funny because information theory has been applied to absolutely every scientific field that we can think of, but Shannon himself says that

the application of it isn't just translating words, although it's really helpful, it's really applying those principles and theory to concrete problems, which is really relevant to what you were just saying, where we need to have a systematic way of thinking, where we put concrete problems that we want to solve into a much bigger system, and then finding an effective way of reducing it back down to something that we can tackle.

So why is biology really a suitable system for this way of thinking is because, first of all, it's very multi-scale.

So we can go from as small as a molecule to as big as evolution that encompasses entire Earth.

And there's many different levels in between that are semi-independent but never totally independent.

And then the second point is that biological systems are highly nonlinear.

So we know a lot about this already.

where people have studied metabolic network and gene networks for a really long time.

And then at the end, there's a lot of very well-motivated problems in biology.

And I think this is really why people tend to think about biology being a playground for quantitative scientists to apply their methods to.

But really, a lot of quantitative methods have been motivated biology problems

such as one of the famous examples, regression.

People never thought about correlating two variables together before they started looking at human height, which is a really simple problem.

Okay, so before we delve into the details, I just wanted to introduce single-cell RNA sequencing for those people who don't know it.

This is a really new technology.

that really started around 2010.

So before that, people knew a lot about transcriptomics, but really when we look at transcriptomics of whole tissues, we couldn't distinguish whether it's like proportion of individual that was changing or each individual that was changing.

So the technology works as such.

So you have single cells in the test tube and basically you merge them

with a single droplet that has unique barcodes for each cell and unique barcodes for each gene.

And then in the end, you get a matrix where every row is a cell and every column is a gene.

And there's many challenges with this technology, mainly because it's really high dimensional.

around 20,000 genes that we can measure for each cell.

Most of them are zero.

And there's really high noise because for each cell there's a tiny little amount of RNA that we're trying to

get a lot of information from.

But what's lucky for us is it turns out that cell biology has a really high redundancy of information.

And people have done this where they measured 100 random quantities from each cell rather than meaningful units such as gene expression.

And they recover almost the same reduction space.

yeah, reduce space as when you measure more meaningful information.

And then it's very robust to a lot of perturbations.

So that's why a lot of these methods work.

OK, so the first example, and my talk is going to be really different.

I'm going to jump over a lot of short examples as to different ways you can compress

a really complex system down into a flat problem that you can't resolve.

So the first example that I'm going to talk about is using variational autoencoder to address this problem.

So it was really good that you introduced neural network before this, because I didn't have time to do that.

But essentially, you have this model that tries to learn a reduced space for single-cell RNA sequencing despite all the technical difficulties.

And one thing that's really different between autoencoder and what we were talking about before

is that when we train a neural network that supposedly gives you some outcome, you have to have training data for what that outcome you expect to see is in at least some of your examples.

But in biology, we kind of want to go in without knowing anything about the system

So all the encoder comes in as a really useful method because you have input on here that's represented by x. And output here, instead of being y, is x prime.

And so going in, you don't need to know what the output is because all your network is trying to do is minimizing the difference between your input and your output.

And what's really useful here is that you can have a middle layer that we call a code that's much, much lower dimension than the initial input.

And because you learn a network that effectively reproduces your input, you know that this code effectively represents all the variation that you care about in your input without ever having to know anything about what actually the relationship is.

But we're going to go back to actually learning about the relationships later, but this is what the essential model looks like.

So SCVI is the method that we're working on.

And it has, I guess, a lot of properties that solves kind of complexity-related problems.

In biology, we have really nonlinear mapping between the gene expression and the cell states, which is what we're trying to work with.

And one way we're addressing this problem as CBI is using neural network.

So by nonlinear, I mean that so we have two cell states, and we have a bunch of genes.

But the way the genes determine how the cells behave

isn't independent.

So like gene one and gene two might have to be together to promote this state.

And then these three genes might have to work together to promote this other state.

And we have a lot of experimental measure, but they're not super, all of them have their own biases.

So we wanted to make something that essentially learns other relationships by itself.

And that's why the autoencoder and neural network approach was really useful.


SPEAKER_01:
It's going to be supervised.

You're going to tell us whether it's got the state of the cell right.

You're going to tell us.

Is it supervised or is it supposed to classify the cell?


SPEAKER_02:
It's not supervised because during the training phase, I'm never telling it what the cell type is.

During the prediction phase, I'm looking at the codes that it provided me.

And I'll ask biologists, do you think this space represents what you think this should be?


SPEAKER_01:
It's inventing some classifications, and then you get it.


SPEAKER_02:
Yeah, sort of.


SPEAKER_01:
So that code space, it's kind of like an abstraction over the potential cell space?


SPEAKER_02:
Yes.

Yeah.

And basically, this is, neural network is a very over-specified, or under-specified problem.

So there's many different solutions that could give you answers that make sense.

And there's a lot of interesting saddle point theory that is relevant in this, but I won't go to it too much.

And then another difficulty that we were presented was the high noise of the data and how the data we observe is probabilistic.

And this is like another layer of complexity over our model of single cells.

So one really, really naive example here is that presumably you have two cells that are completely identical.

All of their properties are drawn from the same distribution.

But when I look at the data, they could look really different.

One could be here, and one could be here.

But that's just a property of my distribution.

It has nothing to do with how different those two cells are.

And so generative model.

We're going to take a generative approach to this.

So another way of thinking about generative model as opposed to predictive model is you can think of predictive model as modeling this probability, py given x. So that's the probability of your outcome given your data.

But generative model actually models both the outcome and the input at the same time as a joint distribution.

And here is the probability model that we ended up using.

So we have a number of variables, and each variable feeds into another variable as an arrow.

So an arrow from z to rho means that rho is dependent on z.

And so what we have is the lane space, which is kind of the biological cell space that we chose to represent in the arbitrary dimension space.

And then rho is the true expression level that we care about.

X is the data that we observe.

And then we have two extra variables that are feeding in.

One is batch, so that's kind of the technical variance that you want to get rid of.

And then there's the scale, which accounts for the cell size.

And combining this model with neural network ends up giving us some really powerful results, because we have a structure.

So we have the function that's highly nonlinear and really free, but then we're making that

like freedom into learning a model that we have a structure for.

And we can interpret a lot of these variables later on.

All right, so that was all.

And then example two is metabolic network in single cells.

So earlier on, we said that we wanted to take a simplification that sort of gets rid of all the prior knowledge that we know about how the network in the cell actually is.

And this is just another approach where we make the known structure of how the genes are related like a focal point.

So this is a metabolic network, basically every dot

is a metabolite, a chemical in a cell.

And then every edge represents an enzymatic reaction that leads from one chemical to the other chemical.

And this is similar to a map where you want to get from point A to point B. And then depending on how fast the road goes, how many people can be on it at the same time, you can optimize your system so the most things goes from point A to point B. Or you can study the system and say what's possible, what's not possible, et cetera.

And so if you want to use this network to study the states of the cell, there's one approach that people are doing is using flux balance analysis.

Basically, we can quantify how thick the edge is by looking at how much enzyme there is in each single cell.

And from that, we can predict what the metabolic state of the cell is.

So is it doing more glycolysis or is it metabolizing lipids for its energy?

Keto diet etc um, and this is like a pretty complex model, because in practice there's not really objective function that you can optimize for like depending on what your cell is trying to do you can think.

Like you can't just assume that your cells just wants to replicates because not all the cells in our body wants to replicates.

Um, so there's like her stick that we can do to this.

And then also it's using a flex balance analysis, which is assuming that everything is in equilibrium, which we know is not true, but this is like the best people.

It's doing what single star and AC.

Can I encourage you to think more about what we can do here?

But, um, this is one example of how we can take another slice at the problem.

And the third slice we're going to take from a very similar problem is instead of using the metabolic network, use the transcriptional network, which is basically how genes regulate each other.

And this particular paper is addressing the problem of development.

So how do cells start from homogeneous states and diverge into very different states?

And how do we measure that potential of divergence?

So the way they did it, this is actually introducing back information theory as well.

What they did is that they took the protein-protein interaction network and then measured the strength of the edges from each protein to another.

And kind of the assumption is that the more entropy there is,

in this network, the more potential it has to diverge into different states.

And they did this experimentally and actually found that they can predict the potential of differentiation just from measuring the entropy from this network perspective.

And then the fourth example is the research is another project that worked on and that's like the simulation perspective, so we have introduced a lot of really complicated methods and models.

And I think one of the ways to test whether our assumptions actually make sense is by doing simulations, coding out kind of the mechanical process that you think things are happening in, and eventually seeing if your model replicates the same behavior as your observations.

Okay, and then example five i'm moving a little bit towards the field of immunology, and this is a really cool study that's about predicting receptor diversity using maximum entropy so for those of you who aren't familiar with immunology um.

We start with the same genome when we're born, but we have a lot of immune cells that each produce a unique antibody.

And the way that it is done in the body is that there is kind of a genetic modification happening in each of your T cells and your B cells where you start with a bunch of

possible fragments, and then most of them gets cleaved out.

So there's the V element, D element, and J elements, and then one draw is taken from each array, and then you end up having one protein that's made up of three elements.

And that is what produces the diversity in your body to make antibodies.

And so D in particular is very diverse because it's not just the combination, it's also genetic mutation that's happening on top of that.

And these researchers are particularly interested in whether you can predict the sequence of D, but so diverse and occupy such a large space that you can't possibly just model the entire space.

So what they did is they came down to a reduction to that space using

physics, theoretical physics, and thermal dynamics, and maximum entropy.

So basically, you can represent the maximum entropy distribution as a function of the effective energy of your sequence.

And so theta is just the symbol.

I forget what it's called.

is a representation of a sequence of sigma, yes, of amino acid.

And then you can write it down as the sum of different sources of energy.

So the first one is just the function of the length of your sequence.

And then the second one is a function of each individual residual of your sequence.

And then the third one is the energy from pairwise interaction from the first residue to the second to the third, et cetera.

And so this is a simplification of what actually happens.

But they were actually able to use this model to predict the actual diversity they see in nature, which I thought was really cool.


SPEAKER_00:
go to diamantis yeah sure all right well resume um thanks for uh making possible this uh his intervention from a very rainy france i'm gonna talk about revolution and complexity starting with some simple facts

the closest thing to a time machine that they know is looking into fossils and- Sorry, not yet on Diamantis, but three or four or five minutes would be good, okay?

Thank you.

Yeah, three slides, three minutes.

The closest thing to a time machine is looking into fossils, as far as I know.

And if we look around us and we can imagine getting into such a time machine and going into the past a few billions of years,

One of the most impressive things we would notice is the complexity of life, no matter how you measure it in terms of different cell types, size, or whatever.

At some point, life on Earth was just blobs, very complex ones, but just blobs.

Look around today, you see wonderful trees, flowering trees.

You see buildings that presuppose a complex civilization.

So what we see is things used to be less complex now are more complex.

Another simple observation in most of the cases that we can actually look at it is the rapid initial increase in diversity.

So we have a great expansion in the possibility space.

Maybe one famous case to think about is the body plans of metazoans.

expanded initially and then we have fewer additions so we can go to the next slide about the mechanisms i have here two figures on the left figure it's a classical figure where the tops

On the left on top, the x-axis is some measure of complexity.

It's very schematic.

And the y-axis is just pure number.

So in the past, in our time machine, we had fewer complexity, but just fewer smaller sizes, smaller numbers.

Today, in the lower part of the graph, we see there's a big tail with a schematic diagram of a human person standing on the right side of the graph, representing the tail of just a random walk with a barrier.

So one proposed mechanism of this

explosion of complexity was just random walk.

We have more than we have.

We tend to focus on the tail of the distribution and then we observe more complexity and that's it.

Not everybody agrees with this point of view of the emergence of innovations or of complex forms.

And this is showing the right hand side, the right graph.

Here time is inverse, so back in time is in the lowest panels and now is in the upper panels.

It shows two different ways of increasing complexity.

Same axis here.

x-axis is a measure of complexity, y-axis is numbers.

So this proposes a statistical test to see if what we observe is just a number of growth numbers, and then we see a larger tail, or the minimum also of the complexity measure growth.

And the third test not shown, but also implemented quite often in data, is a parent-offspring

So if you look back in the fossils or you look into the parents and the offspring, you can measure if in general we have a tendency of growth, of complexity, whichever measure we use, or diminution.

And then you can look into averages.

And then wrapping this up in this third slide, if we can move ahead, there are many opposing views regarding to how

how we get complexity to be evolved, why we have an open-ended evolution, and an evolution of new qualitatively different forms would be called innovation.

And I just list a few here, using one example of the role of modularity, diversity, and robustness, the role of environmental influence.

i wanted to think about this a bit more broadly going into other artificial systems beyond biology we have we can look and we can now measure how a piece of code has been growing we can think of the linux operating system it's been here for many of years it has thousands of hours of work and we can again study complex complexity with different very specific measures and we can make the same

statistical measurements and we can see if the emergence of complex forms of increased complexity is more of a passive trend of growth in terms of volume or active trend or a mixture of both.

And with this I would like to close the three minutes and let you think each your own system, your own expertise, what have been the

the important or maybe the more novelties in your own system, the emergent, the complex systems, how they emerge and what possible pattern they could follow and how we could make this statistical test.

But this is probably what we could do in the project section discussion.