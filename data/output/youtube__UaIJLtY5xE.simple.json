[
  {
    "start": 10.891,
    "end": 31.953,
    "text": " Okay, so what we do at AtomWise, and this actually encompasses all of my personal research as well, which means that unlike our two colleagues, I can't present you with tons of stuff that directly talks about the data that I'm getting out of what I do, but I can sort of provide signs around it to get you an idea.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 33.555,
    "end": 40.799,
    "text": " But basically what we do, I was surprised to look at the demographics that we have as many biologists as showed up here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 42.48,
    "end": 54.687,
    "text": "What we do is we try to detect the affinity of standard drug candidate molecules for specific receptor domains.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 55.248,
    "end": 60.131,
    "text": "So the biologically active receptor domains on medicinally important proteins.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 61.672,
    "end": 65.694,
    "text": " The idea is to do this essentially sight unseen.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 66.955,
    "end": 76.441,
    "text": "We rely on publicly published data from x-ray crystallography experiments.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 77.582,
    "end": 81.444,
    "text": "Basically, if you don't know what this is, you crystallize a protein.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 82.005,
    "end": 83.485,
    "text": "You refine a protein from",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 84.466,
    "end": 84.806,
    "text": " cells.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 85.286,
    "end": 89.268,
    "text": "So you destroy them and get the protein out, usually through centrifugation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 90.088,
    "end": 97.612,
    "text": "And then you clean it up and attempt to get it into a crystal.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 98.552,
    "end": 100.253,
    "text": "So you have sort of a regularly stacked",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 101.053,
    "end": 109.597,
    "text": " arrangement, shoot x-rays at it, capture a diffractogram, so the reflection angles coming off the x-rays, and then a bunch of math takes place.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 110.137,
    "end": 115.76,
    "text": "And out of that, you get an electron density of the location of the atoms inside the protein.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 117.081,
    "end": 117.801,
    "text": "So we take these.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 117.981,
    "end": 120.382,
    "text": "These are actually just mathematical structures.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 121.203,
    "end": 128.506,
    "text": "And we do some trickery called docking to estimate where the most likely position of a candidate drug that hasn't been",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 130.443,
    "end": 136.008,
    "text": " you know, crystallize with this protein where it ought to be inside of its biologically relevant binding pocket.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 136.669,
    "end": 145.397,
    "text": "And then we train a neural network to give us a biological activity based on that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 147.408,
    "end": 153.59,
    "text": " It's a very complex, difficult problem.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 153.71,
    "end": 162.713,
    "text": "We get IC50, usually, or PKA scores from some type of assay.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 162.753,
    "end": 166.995,
    "text": "So IC50 is the inhibition concentration for whatever type of cell you have.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 167.635,
    "end": 190.928,
    "text": " the 50 of them die off within a period of time these are relative and so you have to have scientists manually transform the data but basically the idea is that we try to determine biological activity you can also get pks or pki so these are uh scores that come from like uh kinetics assay like to see you know at what point the activity of the protein uh stops going out so you have like a situation",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 192.629,
    "end": 192.93,
    "text": " Yes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 194.172,
    "end": 196.616,
    "text": "Yes, and the data is split and very large.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 197.377,
    "end": 200.683,
    "text": "We don't mix them together yet, but I will someday.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 202.366,
    "end": 202.546,
    "text": "Soon.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 204.039,
    "end": 204.479,
    "text": " Sunday soon.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 206.821,
    "end": 209.904,
    "text": "So the idea is here, just learning about what I do.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 210.465,
    "end": 212.606,
    "text": "We have this proprietary structural data.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 212.686,
    "end": 216.189,
    "text": "Some of it is public, but we make it proprietary by doing a bunch of action to it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 217.55,
    "end": 222.615,
    "text": "Source experimental data, which we capture through working with labs or just off public domain.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 223.095,
    "end": 223.936,
    "text": "Build the training base.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 224.797,
    "end": 227.419,
    "text": "We do good stocking, which is just fitting the molecule.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 228.92,
    "end": 236.507,
    "text": " There are quite a bit of details in here with how we line up given examples of protein molecule docking structures.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 236.987,
    "end": 241.291,
    "text": "And then we put it through a convolutional neural net, which is basically what I'm going to be talking about today, actually.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 241.331,
    "end": 247.477,
    "text": "That's the topic of today's conversation, which is let's be philosophical about neural nets.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 247.557,
    "end": 249.179,
    "text": "That's actually what I'm going to talk to you about.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 250.219,
    "end": 260.347,
    "text": " If you want to talk more later about the fancy stuff that I had planned, I was going to give you a hint about what I'm working on and that is called tensor field networks.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 260.988,
    "end": 267.733,
    "text": "But I'm not going to do that now because that just would be too much math and it would take a long time to talk about.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 268.093,
    "end": 269.754,
    "text": "So I'm going to start up here at the top.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 270.735,
    "end": 274.378,
    "text": " And we'll go to the slideshow from the beginning.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 274.979,
    "end": 276.58,
    "text": "And now you can see my black box.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 278.922,
    "end": 281.163,
    "text": "Right, so we want to go inside the black box, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 282.104,
    "end": 290.891,
    "text": "We don't want to live in a world where people... So my colleagues here, everybody's into this dynamical systems, Lyapunov stuff.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 291.912,
    "end": 294.214,
    "text": "And I'm surprised at how many people really grabbed onto that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 295.535,
    "end": 296.095,
    "text": "Generally...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 297.356,
    "end": 305.239,
    "text": " For the rest of us, when we use equations to make models of systems, they're usually in the form of machine learning models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 305.279,
    "end": 306.42,
    "text": "That's what's in your phone.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 306.5,
    "end": 308.861,
    "text": "That's what's at Stitch Fix.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 308.921,
    "end": 311.422,
    "text": "That's managing your camera.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 312.942,
    "end": 314.343,
    "text": "So that's what I'm into.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 315.663,
    "end": 322.746,
    "text": "And I wanted to talk about the premier machine learning model of our time, which is the artificial neural network.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 325.007,
    "end": 334.813,
    "text": " I have opinions about whether or not that should be named that and talk about the way that the complexity community has been tackling some of the behaviors of this system.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 336.113,
    "end": 338.455,
    "text": "So other stuff later.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 339.736,
    "end": 348.142,
    "text": " Okay, so to answer Professor Fisher's question, what is complexity?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 348.743,
    "end": 351.745,
    "text": "Well, I think of the idea of complexity as dynamical systems.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 351.765,
    "end": 352.706,
    "text": "That's one way to do it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 353.346,
    "end": 357.629,
    "text": "Another way to do it is to think of it as \u2013 I often think of it as a \u2013",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 359.771,
    "end": 369.435,
    "text": " an emergent behavior that occurs as a result of small autonomous units, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 369.975,
    "end": 374.258,
    "text": "And so you can go and you can get into like the cellular automata, like the game of life.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 374.518,
    "end": 375.698,
    "text": "Have people played that before?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 376.098,
    "end": 376.399,
    "text": "Played?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 376.419,
    "end": 381.281,
    "text": "Don't know if I'm old enough to have done it myself.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 381.781,
    "end": 382.642,
    "text": " Yeah, by hand.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 383.422,
    "end": 385.143,
    "text": "You get little wooden blocks.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 387.224,
    "end": 401.832,
    "text": "Ideally, when you actually build these things, they have this sort of unpredicted emergent behavior, which is sometimes adaptive star and useful star.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 403.293,
    "end": 408.336,
    "text": "And that's kind of how we think of the neural network today.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 410.297,
    "end": 422.22,
    "text": " And scientifically, I think of complexity as not only building this sort of reduced representation of a system, so you get the cellular automata, but using observations to refine it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 422.3,
    "end": 424.401,
    "text": "So Dr. Fisher talked a lot about that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 427.162,
    "end": 429.542,
    "text": "And that's kind of what I was going for today.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 430.642,
    "end": 434.663,
    "text": "So machine learning, a love story, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 438.451,
    "end": 444.517,
    "text": " The idea today is how did we go about getting machine learning from the complexity point of view?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 445.558,
    "end": 460.211,
    "text": "And Tom Mitchell famously described a computer being said to learn with a given experience with respect to some class and some performance measure if the performance in that measure increases with more experience, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 461.913,
    "end": 464.476,
    "text": " Name whatever experience it is, there are many ways to do it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 466.759,
    "end": 471.586,
    "text": "And this started off sometime in the 50s with this idea.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 472.247,
    "end": 477.033,
    "text": "If we want to make an artificial brain, can we make it out of artificial neurons, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 478.034,
    "end": 480.475,
    "text": " So McCullough pits sometime in 43.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 481.856,
    "end": 485.458,
    "text": "I mean, this is like World War II, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 486.218,
    "end": 488.939,
    "text": "It's like literally during the war they're doing this research.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 491.24,
    "end": 492.401,
    "text": "Teach it to beat Nazis.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 494.402,
    "end": 497.503,
    "text": "Okay, so they come up with this model of the neuron.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 497.543,
    "end": 498.804,
    "text": "I think McCullough was...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 500.06,
    "end": 503.883,
    "text": " a biologist and Pitts was a logician.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 503.903,
    "end": 508.966,
    "text": "I'm showing it with three inputs and one output here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 510.327,
    "end": 512.248,
    "text": "The McCulloch-Pitts neuron is very simple.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 513.088,
    "end": 525.737,
    "text": "Basically, it just takes a sum of its inputs and has some function where basically if the sum of the inputs is greater than zero, then it reproduces one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 526.397,
    "end": 528.838,
    "text": " from its function, otherwise it reproduces zero.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 528.918,
    "end": 530.699,
    "text": "So it's basically a yes-no system.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 532.34,
    "end": 538.883,
    "text": "I leave it to the reader to determine if that is an actual, an accurate representation of this neuron.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 540.404,
    "end": 541.004,
    "text": "Where is X?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 543.025,
    "end": 543.985,
    "text": "Where's Y?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 544.245,
    "end": 545.146,
    "text": "I'm not sure.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 547.707,
    "end": 549.288,
    "text": "But we'll continue.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 550.108,
    "end": 551.529,
    "text": "That's a fantastic paper, by the way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 552.647,
    "end": 554.108,
    "text": " That Pekatovich paper, that's great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 554.408,
    "end": 558.851,
    "text": "You should read that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 559.112,
    "end": 568.197,
    "text": "No, they don't call it the Y and X. This is actually an example of complexity being evolved to learn complexity.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 568.698,
    "end": 573.221,
    "text": "So they used a robotic evolution strategy to develop a probe",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 574.386,
    "end": 578.608,
    "text": " to sample electrical currents on the surface of the neuron.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 578.669,
    "end": 591.376,
    "text": "So they actually use this complex, like a control systems approach to develop a molecular probe that lights up when the neuron sends out a signal.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 592.415,
    "end": 595.598,
    "text": " You're trying to get accurate representation, so there's a guaranteed way not to make progress.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 596.079,
    "end": 596.539,
    "text": "Right, yes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 596.559,
    "end": 605.488,
    "text": "How does it retain the hybrid nature of continuous signals resulting in discrete events?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 605.768,
    "end": 607.61,
    "text": "Yeah, that's right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 607.71,
    "end": 611.474,
    "text": "So the big thing is that you have, I mean, it's very long.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 611.514,
    "end": 615.758,
    "text": "The probe sends out one photon at a time, and you use a photon counter, and it's really beautiful.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 616.158,
    "end": 617.759,
    "text": " You can see the transduction of signals.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 618.359,
    "end": 620.98,
    "text": "Let's not get too much into that, but the Bekatovich paper is great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 621.02,
    "end": 622.241,
    "text": "That's why I have the reference there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 624.342,
    "end": 627.083,
    "text": "So this simple, this McCulloch-Pitz neuron is great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 627.483,
    "end": 634.406,
    "text": "It provides some powerful utility, which I think is a good example of building these small automata and having them do",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 635.446,
    "end": 644.932,
    "text": " Good stuff like we can basically get the or and and not functions out of them just by setting You know what the threshold is for G, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 645.072,
    "end": 663.984,
    "text": "So if all of these are greater than one Or one of them is greater than one then you can get or right so this one this one or this one Have a value of one or more than or it's greater than one right or zero more than zero basically they add to one and Then you have an or function, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 664.344,
    "end": 676.787,
    "text": " this or this if all of them sum up to three or greater then you have an and and then here you can have like a switching function where if you have any value in x1 then it sends out a zero",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 678.879,
    "end": 682.782,
    "text": " And we can build these representations into tables, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 683.482,
    "end": 684.363,
    "text": "That's great.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 685.344,
    "end": 688.707,
    "text": "We can even, you know, simulate a whole computer with the McCulloch-Pitz neuron.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 689.367,
    "end": 692.65,
    "text": "But logical circuits are not really adaptive, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 693.21,
    "end": 696.073,
    "text": "I mean, that kind of is just using a computer to simulate a computer.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 697.245,
    "end": 708.709,
    "text": " So getting on to the newer forms of neurons, we have the perceptron, which actually was like a missile targeting tool for a while.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 710.189,
    "end": 712.05,
    "text": "You can build layers of these guys.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 712.93,
    "end": 716.831,
    "text": "And they're pretty good at picking out positive and negative labels.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 718.432,
    "end": 722.753,
    "text": "And they basically work by you build a layer of them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 722.793,
    "end": 724.014,
    "text": "You have some input here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 724.915,
    "end": 731.467,
    "text": " And if the sum of the input is greater than a threshold value, then it sends out a 1, if not 0.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 732.73,
    "end": 735.295,
    "text": "But the real innovation here is that these are trainable.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 737.043,
    "end": 751.227,
    "text": " So if you have an x which falls in the positive label set, and for some reason it's not being classified correctly, you add x to the weight for that particular type of x, that particular variable.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 751.727,
    "end": 754.628,
    "text": "And if it's in the negative, you subtract x from the weight.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 755.268,
    "end": 759.909,
    "text": "And so that's sort of like a first example of an adaptive automaton for this purpose.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 760.809,
    "end": 764.391,
    "text": " And these are great, except they're kind of limited to linear functions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 766.212,
    "end": 777.678,
    "text": "It just, like, you know, if you have to have, if you have XOR, so for example, this function only produces a positive value if these two values are different, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 778.919,
    "end": 782.401,
    "text": "You have to have one plane crossing this way and one plane crossing that way.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 783.361,
    "end": 784.742,
    "text": "The perceptron can't do that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 785.891,
    "end": 788.852,
    "text": " But there is something that can, and that's the modern artificial neuron.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 789.433,
    "end": 795.295,
    "text": "It can't do it individually, but in groups, you can do it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 796.496,
    "end": 800.518,
    "text": "And the big difference here is that we don't fix the output to one.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 801.258,
    "end": 806.621,
    "text": "We actually set the output to be sort of a simulacrum of what a real neuron does.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 809.351,
    "end": 822.178,
    "text": " Which is it has sort of a gradated signal right there is a fully on and fully off But there's also some level in between right and we use this function called the activation function and",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 823.932,
    "end": 830.657,
    "text": " And you make groups of these guys, and you can get something like regression.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 830.757,
    "end": 832.038,
    "text": "So this is a fun thing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 832.098,
    "end": 833.379,
    "text": "I actually have code for it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 834.42,
    "end": 840.725,
    "text": "So say that the long debate over sleep versus study is improving your grades.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 840.765,
    "end": 843.547,
    "text": "The truth is that sleep improves your grade a lot better than study.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 846.81,
    "end": 847.57,
    "text": " During the lecture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 847.951,
    "end": 850.373,
    "text": "Yeah, during the lecture.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 850.453,
    "end": 853.195,
    "text": "Good night of sleep the night before will probably do a lot for your grade.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 854.116,
    "end": 856.058,
    "text": "I mean, assuming that you've done anything for the class.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 856.118,
    "end": 857.219,
    "text": "But anyway, you have some values.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 857.299,
    "end": 859.982,
    "text": "And we want to determine how this all works.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 866.487,
    "end": 870.809,
    "text": " So you put in, you have what's called an input layer of perceptrons.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 871.449,
    "end": 873.13,
    "text": "This is sort of like a virtual layer.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 873.71,
    "end": 877.572,
    "text": "And the value of sleep goes into each one of your perceptrons here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 877.692,
    "end": 879.092,
    "text": "This is, say, your hidden layer.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 880.133,
    "end": 892.198,
    "text": "And the weights are adjusted until we find an accurate output for the amount of grade that you're going to get for a given input of sleep and study.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 894.033,
    "end": 898.396,
    "text": " And this is the basic principle of all deep learning that controls your phone and your car.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 899.737,
    "end": 903.32,
    "text": "The big question is, how do we actually improve these weights?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 903.4,
    "end": 915.068,
    "text": "Because we don't just subtract x. We actually have to subtract the amount that the weight changes as a function of the error that the whole thing produces.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 916.907,
    "end": 920.87,
    "text": " And that process is called back propagation, which I have many long slides on.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 920.89,
    "end": 921.911,
    "text": "We're not going to go over them.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 921.951,
    "end": 923.612,
    "text": "I just don't have the time and neither do you.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 923.812,
    "end": 924.953,
    "text": "We're all too hungry and tired.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 926.654,
    "end": 928.956,
    "text": "But if you want to talk about it more later, we can.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 929.056,
    "end": 934.639,
    "text": "Basically, back propagation uses reverse differentiation to cut down the number of computations that you have to have.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 935.54,
    "end": 937.902,
    "text": "If you actually spend some time looking at it, it's not that complicated.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 938.222,
    "end": 941.664,
    "text": "But anyway, this is a pretty simple system comparatively, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 941.925,
    "end": 942.125,
    "text": "I mean...",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 943.777,
    "end": 950.621,
    "text": " So the titan of the modern era is the convolutional network, which is the thing I've been saying over and over again.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 951.962,
    "end": 957.746,
    "text": "And the big difference between the convolutional net, sorry for not putting up a reference.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 957.946,
    "end": 958.967,
    "text": "I'll make it up to them later.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 960.127,
    "end": 970.194,
    "text": "But basically, the idea is that instead of having flat layers of neurons, we actually have 2D and even 3D layers of neurons.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 971.695,
    "end": 989.155,
    "text": " So these guys consume data in the form of, say, an image, like you would with a protein, say, and they refine them through successive layers of what's called bottlenecking, basically changing the number of weights that each response",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 991.88,
    "end": 992.742,
    "text": " responsible for.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 993.884,
    "end": 1003.621,
    "text": "So basically, by shrinking layers, you enable it to learn a refined representation of the data as it comes in.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1005.993,
    "end": 1007.834,
    "text": " And why is it convolutional?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1007.974,
    "end": 1012.957,
    "text": "It's not convolutional in the traditional sense that we all learned when we were infants where you take an integral over two functions.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1013.958,
    "end": 1021.043,
    "text": "It's convolutional in the sense that you learn these sort of filter representations.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1021.363,
    "end": 1023.645,
    "text": "This is a parameter in the neural net.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1024.125,
    "end": 1028.508,
    "text": " And the filter is sort of scanned over the input image.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1028.548,
    "end": 1029.649,
    "text": "This is the input image.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1029.709,
    "end": 1030.49,
    "text": "This is the filter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1031.41,
    "end": 1033.792,
    "text": "And you basically take the Hadamard product.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1034.632,
    "end": 1036.294,
    "text": "You probably learned that in elementary school.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1037.174,
    "end": 1041.337,
    "text": "And the Hadamard product is just the point-to-point product.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1041.797,
    "end": 1042.478,
    "text": "And you sum it up.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1042.898,
    "end": 1046.181,
    "text": "And that gets you this sort of reduced representation of the field above it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1046.281,
    "end": 1048.942,
    "text": "So modern nets actually learn the filter.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1049.303,
    "end": 1052.185,
    "text": "And the filters actually are what represents the layers.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1052.725,
    "end": 1060.892,
    "text": " closer to sort of the discrete signal processing version of convolution but applied over sort of tensor or like higher up?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1060.992,
    "end": 1062.633,
    "text": "It sounds like what you're describing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1062.674,
    "end": 1064.395,
    "text": "I'm trying to make the link.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1064.795,
    "end": 1068.899,
    "text": "It's actually sort of like the chaotic process inside of a human brain, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1068.939,
    "end": 1072.362,
    "text": "You have an optical nerve that consumes something and then it",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1073.242,
    "end": 1086.172,
    "text": " It goes through, I mean, if you can map the inputs, it goes through different representations as the input passes all the way back to your, you know, the back of your, what is that, medulla or whatever?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1086.192,
    "end": 1087.694,
    "text": "Yeah, that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1088.214,
    "end": 1089.515,
    "text": "So your occipital lobe, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1089.535,
    "end": 1092.878,
    "text": "Your occipital lobe processes it and then refines what you're seeing.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1092.898,
    "end": 1095.079,
    "text": "I have an image of that somewhere.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1095.5,
    "end": 1098.702,
    "text": "But basically the idea is that you, your brain",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1102.779,
    "end": 1112.124,
    "text": " and I'm not even sure that this is how it was developed, but your brain learns refined representations in very much the same way that the convolutional neural net does.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1112.244,
    "end": 1113.625,
    "text": "So this is the model of the brain?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1113.645,
    "end": 1114.325,
    "text": "Yes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1114.885,
    "end": 1115.606,
    "text": "Kind of.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1115.766,
    "end": 1118.407,
    "text": "It's more like a model of a model, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1118.867,
    "end": 1123.55,
    "text": "And so I don't want to go down that road quite yet.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1123.89,
    "end": 1126.651,
    "text": "I think that that's more of the issue here.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1126.671,
    "end": 1131.734,
    "text": "Would it be fair to conceptualize that as a multilayer volumetric version of a suborganizing map?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1134.136,
    "end": 1141.558,
    "text": " Except the self-organizing map allows it to have, I think self-organizing maps allow free association between units, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1141.578,
    "end": 1144.539,
    "text": "Because that's the next step between that and a neural gas.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1145.379,
    "end": 1147.94,
    "text": "So we're talking about much older models.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1149.1,
    "end": 1153.081,
    "text": "This is like a pre-organized self-organizing map.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1154.082,
    "end": 1160.943,
    "text": "You're giving it limits within which each SOM can possibly develop a representation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1160.963,
    "end": 1162.524,
    "text": "That's further way down, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1163.365,
    "end": 1168.391,
    "text": " So for those of you that are a little left behind, just imagine that at the end we have to make a prediction.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1169.032,
    "end": 1176.78,
    "text": "And so as it compares between the output of the net and its prediction, the weights are changing at each level.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1176.8,
    "end": 1180.725,
    "text": "And I'm kind of going to get to that in a minute.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1182.583,
    "end": 1189.068,
    "text": " So just this very simple process has led, and I could spend 25 minutes on this slide.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1189.108,
    "end": 1189.388,
    "text": "I won't.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1190.028,
    "end": 1196.633,
    "text": "But this very simple process has led to tremendously frightening outcomes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1196.893,
    "end": 1200.856,
    "text": "So this is the state of neural nets today.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1200.876,
    "end": 1203.939,
    "text": "This thing is much smaller than I thought it would be.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1204.019,
    "end": 1206.821,
    "text": "But basically, this paper is absolutely stunning.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1208.502,
    "end": 1223.354,
    "text": " This data set that they're training it on is called the Clever data set and basically it asks the machine a question in English and the machine is asked to select out and learn however, you know, whatever question it is.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1223.374,
    "end": 1225.356,
    "text": "So here you guys in the back can't possibly see this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1225.996,
    "end": 1233.382,
    "text": " They're asking the machine, what size is the cylinder that is left of the brown metal thing that is left of the big sphere?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1235.423,
    "end": 1236.424,
    "text": "And it nailed this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1237.064,
    "end": 1243.91,
    "text": "So this particular one solves this set at 95.5% success rate.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1244.27,
    "end": 1248.093,
    "text": "Like it can pick it out and this particular machine can even make measurements.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1249.162,
    "end": 1249.542,
    "text": " on these.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1253.066,
    "end": 1255.308,
    "text": "This one here, Google's so into this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1255.448,
    "end": 1256.509,
    "text": "I don't know why they're so into this.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1256.569,
    "end": 1258.09,
    "text": "This is called a generative network.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1258.991,
    "end": 1262.734,
    "text": "They spend a lot of time on it.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1263.435,
    "end": 1264.596,
    "text": "This one is really impressive.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1264.636,
    "end": 1266.277,
    "text": "They ask it to produce a description.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1266.357,
    "end": 1267.838,
    "text": "So you write a description of a bird.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1269.08,
    "end": 1271.762,
    "text": " An all-black bird with a distinct, thick, rounded bill.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1272.323,
    "end": 1283.194,
    "text": "And then the machine actually generates pictures of birds, generated pictures of birds, that are photorealistic to the human eye that fits that description.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1283.214,
    "end": 1284.416,
    "text": "Okay?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1284.436,
    "end": 1287.339,
    "text": "It generates a much less variable...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1288.109,
    "end": 1289.049,
    "text": " I'm sadder than personally.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1290.75,
    "end": 1291.05,
    "text": "Yes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1291.55,
    "end": 1291.99,
    "text": "Yeah, right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1292.831,
    "end": 1293.291,
    "text": "Exactly.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1293.311,
    "end": 1294.611,
    "text": "I think you're hitting exactly.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1296.032,
    "end": 1296.352,
    "text": "Yes.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1297.552,
    "end": 1299.833,
    "text": "And that's kind of where I wanted to go.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1301.353,
    "end": 1302.414,
    "text": "So let's keep that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1302.434,
    "end": 1303.554,
    "text": "Yeah, one more minute.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1303.614,
    "end": 1303.974,
    "text": "Keep that.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1303.994,
    "end": 1304.715,
    "text": "Oh, please.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1304.795,
    "end": 1305.315,
    "text": "Come on, man.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1306.76,
    "end": 1308.622,
    "text": " Can I borrow like three minutes or four?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1308.642,
    "end": 1310.003,
    "text": "All right.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1310.603,
    "end": 1312.825,
    "text": "Just let me get through the jam.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1312.965,
    "end": 1317.289,
    "text": "So this raises the larger question.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1318.37,
    "end": 1321.793,
    "text": "If convolutional neural nets are the answer, what is the problem?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1322.293,
    "end": 1333.042,
    "text": "Because this paper, this is a recent Complexity Crew paper, and they go inside and they're actually measuring the gradients, the changes in weights, as the network is learning.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1334.821,
    "end": 1344.966,
    "text": " And as you make the network more deep and more powerful, so the more like Colossus this network is, the more like white noise the gradients start to look like.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1346.647,
    "end": 1351.129,
    "text": "So the gradients themselves, right, the changes in weights are looking like white noise.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1352.389,
    "end": 1355.771,
    "text": "So that makes us wonder what exactly is the network learning?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1356.744,
    "end": 1359.626,
    "text": " Right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1359.866,
    "end": 1361.927,
    "text": "This crew here, fantastic paper.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1363.968,
    "end": 1370.112,
    "text": "These guys are using a mutual information approach to study the behavior of the gradients inside the network.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1370.933,
    "end": 1384.181,
    "text": "And so we're low on time, but basically the idea is that they have, they basically use this mutual information mapping to measure how well the next layer down does at representing the layer before.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1385.141,
    "end": 1389.688,
    "text": " and also against the first input using mutual information.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1390.689,
    "end": 1393.332,
    "text": "And we get these relatively complex trajectories.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1393.433,
    "end": 1399.481,
    "text": "So as we go from cool to warm, that's the number of training cycles that the neural network has gone through.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1400.242,
    "end": 1402.505,
    "text": "And actually what we see here is that the network",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1403.266,
    "end": 1407.191,
    "text": " So this is the trajectory and learning the original representation.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1407.812,
    "end": 1411.076,
    "text": "And then this is the trajectory and learning the next layer down.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1411.837,
    "end": 1414.801,
    "text": "And actually what we can use, you go up, that's the layer number.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1415.441,
    "end": 1418.866,
    "text": "Actually, what you see here is that the network first",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1419.366,
    "end": 1424.612,
    "text": " learns a lot about what you're putting in and then starts to forget as it curves back.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1425.412,
    "end": 1430.878,
    "text": "And as it reaches its final optimum, this is in some place that's not much like the way a human thinks, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1431.299,
    "end": 1436.984,
    "text": "Or at least the way you think you think, which is sort of getting to Dr. Fisher's point",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1437.765,
    "end": 1441.286,
    "text": " which is that, has this story really been a success?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1441.366,
    "end": 1444.507,
    "text": "I mean, have we actually modeled a human neuron?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1445.148,
    "end": 1447.868,
    "text": "Could the pathway to this level of success have been shortened?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1449.209,
    "end": 1453.25,
    "text": "And as a bigger picture, what can we do to refine this reduced representation?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1453.75,
    "end": 1455.071,
    "text": "How meaningful is this really?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1457.653,
    "end": 1466.904,
    "text": " And to my friend here in the front, is a mind really the same thing as reasoning, right?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1466.984,
    "end": 1470.789,
    "text": "Did we actually build a brain with this stuff even though it appears to be reasoning?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1472.205,
    "end": 1479.11,
    "text": " Or is this actually poking at some underlying structure, which is described apparently by white noise?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1480.952,
    "end": 1483.814,
    "text": "So that's sort of my stump speech there.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1483.934,
    "end": 1491.259,
    "text": "So I leave it to you to decide if we have modeled neurons.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 1491.84,
    "end": 1494.322,
    "text": "I think that the questions are big and still out there.",
    "speaker": "SPEAKER_02"
  }
]