SPEAKER_00:
Sean called it complex thermostatistics.

And I didn't really know what to do with that, except that there's complexity.

And everyone's talked about complexity so far.

But I'm going to try to give a definition which roots things and then helps with the narrative.

Because there's like, well, everything's complex.

And there's lots of stuff, so it's complex.

And there's big numbers.

OK.

So then there's thermodynamics, statistics, statistical mechanics, condensed matter physics, soft matter physics.

Is high energy physics complex?

OK.

So I'm going to go through all these things in one slide at least.

OK, so first me.

And so Sean said who I am.

And then the way I like to say who I am is I learn quickly and see complex problems.

So OK, there's complexity there.

And I like to do multi-scale physics modeling and prediction with varied levels of complexity.

And this definition of complexity, where I say back of the envelope versus computationally intensive simulation, actually the definition I'll give makes sense in that sort of single phrase.

OK.

And then I do lots of research.

So right now I work in advanced computing, like massively parallel classical, and also looking at quantum computing and in AI and ML.

And then I've also worked in material science and dynamical systems and physics and robotics and all this other stuff.

So real quick slide on what's the connection between thermodynamics, statistical mechanics, dynamical systems, and complexity.

So these guys, this spans like two centuries, right?

Boltzmann and Ken Wilson.

And so way back then, they started studying thermodynamics, which was like a set of axioms at a carrier level.

And they came up with a consistent theory, which started at that level.

which agreed with the underlying or top-level thermodynamic axions.

But I think the important part is that they did all this work, so we're talking about ideal gases and phase transitions, but they extracted a whole bunch of useful tools from this study that goes on to be used in other complex fields.

I think that we've gotten a taste of that so far.

So I think this is, for me, the one pager extraction or take away from Thermo and StatMac and all that is the tools that you got out of it and how much they're applicable in other domains.

Not necessarily exactly, but things like phase transitions.

OK, so for me, it's like phase transitions and self-similarity and universality and divergence and unpredictability and Lyapunov exponents and attractors and bifurcations and limit cycles and all this junk in here, which you can find in books like Hilborn.

And you can find Strogatz and Sprott, and then these are all good ones for StatMech and condensed matter physics as well.

Pothria and Macquarie and Ashcroft-Merman and all that.

Okay, so I would say that this is my definition of complexity and thermodynamics and all that, is what's contained in these texts.

So now I want a good definition of complexity so that we can compare different systems, because I want to look at lots of different systems, like simple systems that might have complexity, the entire universe and reality.

So the definition of complexity, which I like, because it lets you play around with those different scales, is Kolmogorov complexity, which is just the length of the shortest computer program in some language that produces an object as an output.

This is the Mandelbrot set, right?

And this thing can be petabytes in size.

You can keep iterating on it forever and make it a very complex object.

But it was made from this thing.

And this fits on a single piece of paper.

This is the metal board set itself, its definition.

This is the kernel.

That's what it produces.

This is a very important concept that is pervasive across the different domains of complexity, is this notion that you can have this very, very complex behavior, but it's coming from some very simple kernel.

And then there's ingredients with the kernel, like the kernel's nonlinear, and if you have the right value of C, then you get that.

And so this was typified in the beginning talk, where he said, look, it's not an Ising model, it's not a spin glass, it has these very particular rules, and that's what gives me the complex behavior that I observe.

So these are the important parts, like what's the necessary ingredients, like non-linearity or maybe degrees of freedom?

And how can we look at simple kernels like this that make incredibly complex objects?

And so for me, my background also is in atomistic modeling.

And so a nice way of bringing the Kolmogorov complexity into atomistic modeling is to say, so we have the Dirac equation, the many-body quantum mechanics problem, which is an NP-hard problem.

Its Kolmogorov complexity is really, really high.

And then you win Nobel Prizes.

First, you win a Nobel Prize for getting the Dirac equation.

Then you get a Nobel Prize for making the Dirac equation really easy to solve.

This is a density functional theory.

And then you have like f equals ma.

So the point is that the complex is high up here, and it's getting simpler as we're coming down.

The computational complexity is coming down, and I would say the complexity is coming down.

OK.

So this is like my background.

So Jared, protein folding.

What else?

You too.

OK.

All right, so I think what's more provocative is to think about, so this is not complex.

This is more complex.

And then reality is very, very complex.

But the same theme is there.

So you have standard model Lagrangian.

If we figure out how to put together with general relativity, then you get the whole universe.

This is the kernel.

That's the object produced.

The Kolmogorov complexity of the universe, incredibly more complex than Mandelbrot's set or the modeling I was showing you before.

So what's very interesting is Seth Lloyd, back in 2001 or something like that, he computed the computational capacity of the universe and estimated something like,

10 to 120 ops on 10 to the 90 bits.

That's a lot of bits and ops.

Also interesting to think about in the context of computation is the Bekenstein bound, if anyone's ever heard of that.

So if we're given volume of space and energy mass in it, how much computation, how many bits can that store?

And so we're really, really far away from the Bekenstein bound with our current computational capabilities.

So the Bekenstein bound leaves room for a million bits in a hydrogen atom.

That's crazy.

Okay, so I'm just trying to set the sort of stage.

Okay, here's an example of one of the papers that's kind of like a seed paper in the track.

And this is quantifying the rise and fall of complexity, and in this case, Kolmogorov complexity, in a closed system.

They call this the coffee automaton.

So in this study, what's very interesting about it is that, so this is the simulation they did, and we don't have to get into the details, but this is the time step, and this is entropy in the blue.

But in the green, you see complexity has a local maximum, and then decayed.

So I don't want to go into any more detail than that, but this is the kind of thing that we could get into in the session.

And in fact, I got into something similar to this back in my master's work.

So this is a turbulence on the surface of the tank.

And over here, we have the initial distribution of, let's say, some particles you blanket on the surface.

And then as time goes on, you see this thing

the entry actually decreases, but the complexity definitely goes up.

I didn't measure what they did in the paper in this study, but you get the idea.

Then these systems that are not in equilibrium, the entry doesn't need to increase.

In this case, it doesn't.

It actually decreases.

And the complexity definitely goes up.

So this is one of the sort of ideas that we can play with in the session.

Another one which is related to the reality of the computation is along the lines of Jared's stuff with deep learning, which is why does deep and cheap learning work so well?

So one idea is that it works so well because reality itself is composed of a sort of finite list of functions which are important to reality, like waves and exponential functions and so forth.

So we could also get into this, although that starts to overlap with what you're doing.

But this is a very, very provocative paper.

Another thing I want to point out, which is I think pretty cool, is thinking about what drives all this really interesting complexity in the geosphere is actually a very simple thermodynamic principle at the highest scale.

So you have the sun.

And it's at 5,000 Kelvin.

And if we take blackbody radiation, the entropy of the sun

Actually, those are going over T. So the entropy of the sun is much lower than the entropy of the re-emitted radiation that Earth re-emits, the difference being the change in entropy which drives all of this and other stuff.

But the point is that thermodynamics at the highest scale sets the stage for all this stuff that we're talking about.

I just wanted to put that slide because it took time to make it, and I think it's nice.

It's poignant, isn't it?

Okay, so I think that's really cool.

That's why I have the slide there.

Going further, so that's at one scale, the highest scale, above the geosphere.

Inside the geosphere at the molecular level, you've got a situation like Kinesin, right?

And these guys are walking in the cell membrane on the scaffolding, on the microtubules.

And they're just able to do their job well enough to beat the thermal fluctuations in the environment.

So they can do work that's corresponding to like two or three times kVT.

So they just sort of get the job done.

And then the whole cell works really well.

But thermodynamics is playing a major key role.

And not even non-equilibrium thermodynamics.

So that's another line of research we could go into.

Another really interesting connection is what's the possible link between intelligence and entropy?

So this is a paper by Wisner Gross talking about causal and tropic forces, systems that you can subject to these causal and tropic forces tend to reach a state which is of maximum potential.

Maximum possibilities.

So one of their examples is a pendulum, which they get to stand up on its end due to these causal and tropic forces.

And what's interesting about that is that that's the sort of point of maximum possibility.

The pendulum could go either way.

It's unstable and so forth.

And then Jeremy England at MIT has a kind of similar line of paper talking about the statistics of self-replication.

and all of this being driven by entropy.

So it's almost as if to say, in an entropic universe, are you sort of bound to get intelligence if the universe is big enough and comets don't blow everything up and all that sort of thing?

In other words, if we have a universe with no entropy, do we not see intelligence?

So this is very interesting work.

Oh, interjecting real quick about me.

So I also do quantum computing.

And just to show you how many connections you can make with thermodynamics and complexity.

So quantum computing is about using qubits as quantum computers.

And I'm using this stuff to try and figure out how to do NP-hard problems faster than classical computers like Traveling Salesman.

And there's this notion of computational complexity, algorithmic complexity, problem complexity, in terms of P and NP, if you've heard of this stuff.

And how much of this region, BQP, which is quantum computation, how many problems can be solved efficiently in this BQP space?

But what's interesting, actually, is Leonard Susskind, who's at Stanford, recently made this conjecture that nature is the fastest scrambler of quantum information and uses a quantum circuit model to do this analysis.

So again, is everything complexity?

Is everything thermodynamics?

So this is another line of research that we could get into.

How much time do I have?

I don't want to go too long.

I don't want to be wrong.


SPEAKER_01:
Let's probably transition onto this in the last part, which is give you kind of the tooling for it.


SPEAKER_00:
Good.

OK.

So that was the end of it anyway.

So I just gave you like six things, I guess, that have connections to thermodynamics, systems in equilibrium and non-equilibrium.

I tried to give you my definition of complexity because I think that that sets an even ground for all of these definitions or people saying complex and complexity.

I think we all know what everyone means when they say it.

But to me, that makes things very, very concrete.