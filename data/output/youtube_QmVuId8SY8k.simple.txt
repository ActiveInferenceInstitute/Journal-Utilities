SPEAKER_02:
spent the morning at Kremlin and its museums, and I've never seen so many beautiful things in my life in one day before.

So, to thank you, I'm going to show you my most beautiful formula for mathematical equations.

And we're going to spend the next two hours deconstructing them.

I'm joking.

I have been told that not everybody here is a mathematician, so I will be showing you my favourite equations, but I'm using them as pictures to remind me what to say and just to rehearse the formal structure of the arguments.

This lecture is really an invitation

for us to think about what the brain is doing.

And I'm going to take the perspective that the brain is optimizing something.

The question is, what is it optimizing?

So I'm going to ask you to stand back from psychology, or Qi learning,

statistical physics, a variation of the energy, but it has a very different meaning and interpretation for us.

And so what I want to do is to walk through the interpretations and the meanings from a psychological, cognitive, and sentient perspective.

Further to that, if we understand the mathematical principles of higher-grade function, or indeed just perceptions and action,

then it should be possible to shine a new light on message passing in the brain, on the aeronal dynamics, on the anatomy of the brain, the functional anatomy or the computational anatomy.

So much of my lecture will be trying to rehearse the general idea in somewhat abstract terms, but illustrating how far one can take

the empirical brain responses that a lot of us measure.

Say, for example, the EEG, or electric magnetic brain responses, or indeed just our choices, our decisions, or our behavior.

So the agenda is ambitious.

I'm going to address a very deep and big question, what do our brains do, and provide a potential answer.

but use the answer to illustrate to you how far one can get in stimulating our understanding of quite high-level or sophisticated behaviours.

I want it to eventually end with an example of reading and language understanding and its brain response correlates.

So that's where we're going to try and go.

If I take too long,

you're going to stop me, and then we'll have a conversation, which is usually the best part of these lectures, is the questions and answers.

So I'm not going to talk for more than an hour, hopefully, and then I look forward to having a conversation about some of these questions.

Now, have you all read this?

I will be asking questions.

This is what we're going to be talking about.

I'm going to introduce the question at hand and then provide this answer in terms of something called active inference, closely related to things like active learning, machine learning, or active vision in the visual sciences.

This is putting perception

in an inactive context that we are actively perceiving and we are in charge of the way that we palpate or sample the world in order

existence.

And I'll explain that connection in about five or six slides.

All of this self-evidencing, this active perception, this active inference, rests upon predicting worlds actively, having a model

And that's known as a generative model, a model that generates sensory consequences from the causes out there which we want to understand.

So I'm going to spend some time illustrating the kinds of generative models that we use to simulate behaviours either in animal studies or in economic games, and the resulting or ensuing belief updating that we can then look at as if we were electrophysiologists or brain labours.

And that's from the principles to the process theory.

So making a distinction between the principles of optimization that we're going to be talking about, active influence and self-evidencing, to the neuronal processes that are occurring in your head at the moment that we can measure as neuroscientists and cognitive neuroscientists.

I'll briefly rehearse some empirical predictions.

I'll go through this quite quickly because I want to get to the end, which is

some of the most advanced applications of these ideas to understanding functional architectures of the brain.

And that's the use of deep generative models that have a deep hierarchical structure, both in terms of abstraction, but also in terms of time.

Exactly the sort of models that you would need to generate language.

And I'm going to turn that on its head and say,

I understand language and I'm going to present some very simple simulations of reading and show that they produce the same electrophysiological responses that we use in classical paradigms like the mismatch negativity or P300 paradigm.

So let me just start very abstractly and very simply.

Imagine you are an owl.

You are a bird of prey.

and you're hungry.

So what are you going to do?

Boris, what are you going to do?

Search.

Perfect.

In that one simple answer, search, there is a whole range of hidden truths

So here's the owl searching and there's the unhappy prey who's about to be eaten by the searching owl.

So clearly, if I'm hungry, the first thing I'm going to do is to resolve my uncertainty about where the prey exists, where here the mouse exists.

And it's that notion of searching, resolving uncertainty, that underwrites everything I'm going to talk about today.

It's important because there are two ways you can write down, and here are the first of the formula, but I repeat, please do not worry about the maths, they're just here to remind me about different ways of thinking about things that you can write down in computer code or in deep mathematic things.

There are two ways you can write down the objective for living.

First of all, we could write down an expression for things that we do, control variables in engineering, which we'll call humor at this point in time, as maximizing the value of some states of the world if I did that thing.

And then what I would be able to do is to create a policy pie

that for any given current state, if I apply this action, I will move to the next state, and then I will maximize the value of being in that next state.

So this is the notion of a value function of states giving rise to state action policy.

It requires you to believe in and commit to the idea that for every state,

there is a label which tells you how valuable that state is, and then all you have to do is to choose the action that takes you from this state to the most valuable state.

But that's not going to work to explain searching.

For the simple reason that if searching is all about reducing uncertainty,

then we know immediately, because uncertainty is an attribute of a belief, it's not an attribute of a thing, it's an attribute of a belief about something, then the function that we need to optimize has to be a function of a belief.

And if the belief is about something, then that's a function of a function of something.

Mathematically, that's called a functional.

So what we actually need, or the alternative way of viewing this, is that our best action at this point in time maximises a functional

of beliefs about states of affairs, states of the world, if I did this action view here.

And I'm going to describe this belief in terms of probability, posterior probability distribution over different states.

Again, don't worry about the maths, just remember the Q is a probability belief, and that

gives a very different sort of optimisation scheme.

So we've moved from a value function of states of the world to a function of beliefs about the strings of the world.

Furthermore,

The notion of searching tells you something else very important.

It means that it matters whether I search for my brain and then I eat it, or I can't eat it and then search for it.

So time and order really does matter, which means that you can't just wipe down an objective function as a function of beliefs.

You have to

define a policy which is a sequence of actions in a particular order.

So now what we have is a notion that there is a best policy that entails or prescribes a sequence of actions in a particular order.

In machine learning or control theaters, there's a sequential

is we know that the best kind of policy, high style, maximizes the sum of this functional, and I'll say now this is a free energy functional, of beliefs given that particular policy.

And that if I don't like policy, I can select the right action.

So these two contrasting ways of writing down formally

what things, what living things do, emerge in many different guises, in many different contexts.

So the notion that you can explain behavior in terms of optimizing a value function rests upon an optimality principle

Lots of examples, optimal control theory, dynamic programming, deep reinforcement learning, utility theory and economics, backwards induction, and so on and so forth.

Some of these you may have heard of, some of which you don't, but they all have in common

this commitment to a value function that can describe everything that we do in terms of an optimisation.

The other approach, which is the approach that we are going to pursue, is based upon the principle of least action.

So action here is just basically a time integral, or a time average of an energy, and I've just said F is a free energy.

So this path

and we're going to maximize that action so that we choose the optimum action.

And this is a free energy principle, also known as active inference.

From this, we will hopefully demonstrate later on artificial curiosity and intrinsic motivation in robotics emerges.

We can also cast this in terms of basic decision theory that I mentioned before.

This is an aspect of sequential policy optimization.

Now I've deliberately sort of contrasted classical value function with three energy functions to highlight the fact that these approaches are belief-based and these are not.

But I'd like to show that they come back together again.

So this becomes this when we remove uncertainty.

So I'll try and describe how we get back to expected utility theory in a few slides.

this quantity is, and I'm going to give you one answer, and I'm not going to motivate it, there is a deep backstory here from statistical physics and phasing mechanics, but I'm not going to worry about that, I'm just going to tell you what it is, and then hopefully convince you it is a suitable objective function by a series of examples which will be in the rest of this lecture.

So here's the basic idea, this quantity is known

quite closely related to something invented here, Kolbroff complexity, or multivitamin complexity.

It's also known as an evidence-low-boundary machine learning.

In statistics, it's known as the log model evidence, also known as marginal likelihood.

You can forget about all these different names.

The reason I'm listing them forward is it's a very, very important quantity, which you see in nearly every field.

and we'll see why that works in a moment.

From a statistician's point of view, this negative physical free energy here, this negative evidence, or not evidence, or evidence lower bound elbow in machine learning, is just a probability of getting these observations owned at this point in time.

given a model of how these outcomes were generated.

And I'm going to read that one.

From a statistician's point of view, you can always write this quantity, this evidence, as complexity, this negative evidence, as complexity minus accuracy, or log evidence as accuracy minus complexity.

What that means, though,

as a statistical organ, an organ that's trying to make inferences just in exactly the same way that human scientists try to make inferences about differences between one group and another group using a t-statistical and analysis of covariance.

The brain is doing exactly the same thing with its sensory data.

It's trying to test different hypotheses, different beliefs about how those sensory data were caused,

And it's doing so by maximizing evasive model evidence, which means that it is trying to find the simplest, minimally complex explanation that provides an accurate account of the sensory data.

And that's going to be very important.

The complexity part is going to be very important.

So it's not just finding an accurate account of data.

It has to be parsimonious and simple in the sense of what can be raised up.

So this variation of free energy is just the mathematical expression of this mixture of complexity and accuracy.

And we imagine that the brain just organizes, learns,

It's complexity minus accuracy or maximizing accuracy and minimizing complexity.

And if that were the end, then that would be a perfectly suitable form of counter-perception.

But what we're interested in here is how the brain couples back and actively samples the data that it

And that's the really important part of it.

So we've already said that we have to define the problem in terms of sequences of actions or policies.

And what we're going to say is that we're going to select those policies that maximize the expected free energy after performing that sequence of behaviors.

So what that means is we're going to effectively choose policies that minimize complexity expected following an action and minimize accuracy, sorry, maximize accuracy following an action.

But notice now the outcomes.

are now random variables.

They haven't yet occurred.

They are in the future.

So now we have to take an average over things that could happen in the future.

So now we're talking about the average complexity, and that turns out to be risk.

Risk, and this is where the equations remind me to provide the formal definition.

So risk is really the needs about what will happen if I pursue this policy.

compared to what a priori I prefer to happen.

So I'll say that again.

Risk is the divergence or the difference between what I think will happen if I do this and what I prefer a priori to happen.

So here are my five beliefs about the sorts of outcomes that I encounter.

warm, having my temperature within the physiological range.

All the things that make me me, and a good me, and a happy me, they are my a priori, my prior beliefs about the outcomes that I will attain if I pursue this policy.

And the goodness of the policy corresponds to the minimum

the reducing the difference between what I think is going to happen and my prior practices.

That must be, that's called risk, and we'll see another instance of that from an economic perspective in a moment.

At the same time, I'm going to maximize my expected accuracy.

So what would that look like?

What does the expected accuracy look like if I haven't actually got the observations at hand?

Well, what it means is,

I am going to deliberately choose policies that make the sensory data as unambiguous as possible.

So, for example, if I walk into a dark room,

I'm going to turn the light off.

Because that's a policy, which means that I can unambiguously see what's going on down there, so to reduce the uncertainty about what could be causing those sensory pressures.

There's a table in front of me, there's a light over there, that I would not be able to see in an ambiguous sensory context if the lights weren't going off.

So this is a little bit like the joke about the man who is drunk and is searching for his keys.

And he's searching for his keys out at the lamppost or the streetlight.

And then someone asks him, what are you doing?

I am searching for my keys.

Did you drop them there?

No, I dropped them over there.

So why are you searching here?

Because I can't see over there.

That's a perfectly Bayes-optimal response.

And it reflects about that we were part of the drives of our good policies and those which minimise our rigidity or maximise expected agency.

And then once we found the good policy, we'll sound an action from that, the action will change the state and the world out there beyond our sense of shame.

Markov blanket.

And that world will then supply new observations, as I will do our perceptual synthesis again, find the simplest action explanation of what's going on, use our beliefs about states of the world to roll out a simulator, another future, another policy, select the policy that minimizes the risk and ambiguity, select the action, and so the perception action, or the action perception cycle continues.

on and on and on, all in the service of minimizing risk and ambiguity, minimizing expected surprise or negative free energy,

future, where that uncertainty includes preferences that I feel familiar with.

So that's the basic story in terms of what is this functional of beliefs that we want to optimize.

This is a horrible slide if you don't do maths, but again, please ignore the equations.

I just wanted to show

and end up with a formalism that people have been working with for centuries or at least decades and decades.

So if you are a mathematician or a physicist, you will now recognize why this quantity is called free energy.

It's basically an expected negative lot probability here, which is called an entropy.

And then this quantity, if I put them together, is the energy.

So it's basically the difference between an energy

and entropy, so it's the energy that's needed to do the work, hence free energy.

But just by shifting these things around or grouping them together in a different way, we can interpret it in terms of complexity and accuracy, as we've just described.

So all I'm saying here is that there are different ways of interpreting these quantities depending upon the words that you use, the constructs that you're taught and you use in conversation with your colleagues.

They're all equally viable.

Another nice example of just switching things around

means there's another interpretation of splitting this uncertainty, minimizing capacity of good policies, of expecting the energy G here.

We can actually carve it or decompose it into another pair of quantities called epistemic value and expected value.

So let me show you how that works, or more precisely,

Let me show you how people have already been using these constructs, these quantities in their work before.

If you just focus on these two terms here, what this corresponds to is essentially the expected difference between beliefs about what's going on out there if I had some observation in the future.

relative to the beliefs about states of the world without those observations.

So what that means is this corresponds to the salience of the policy or the action.

It tells me the amount of uncertainty I am reducing or the amount of information I have gained if I looked over there as opposed to looking over here.

So this quantity has been used a lot in visual neuroscience, has found visual searches and salience maps in terms of the best place to go and sample the world from.

It's the place that minimizes your uncertainty, or that's invited your information gain, that has salience, epistemic performance.

It's also mathematically exactly the same as the mutual information, or the mutual predictability, or the shared variance

between the causes, states of the world, and the consequences, the outcomes that are generated by those states.

So effectively, what we're trying to do is to move and palpate our world, either visually with eye movements, or literally with our skin receptors, by seeing, for example, the layout of a new hotel room in the dark.

You're testing hypotheses to feed your way around

You're sampling actively those sensations that tell you, ah, no, this is a table, not a bed.

But you have to have those hypotheses in mind in order to reduce your uncertainty over the hypotheses that you are entertaining.

And in doing that, you're increasing the mutual information between what you feel and what caused those feelings.

So that's a very important aspect of this

bit simpler.

So now I'm doing what I promised before.

I'm going to get back to the value function.

But if you remember before I said the difference between the value function and the free energy functional is that one is belief-based and the other is not.

I can actually convert the belief-based

So the first sort of uncertainty I'm tripping with is basically ambiguity.

I'm going to assume there are creatures out there that can see every hidden state of the world.

There's no sensory noise, there are no hidden states of the world.

And what I see in my sensory organs, my observations, my mantras, are the states that I'm observing.

So S's become O's and O's become S's.

What we are left with is just the divergence of the difference between the predicted and preferred outcomes.

It's just our risk again.

So this is risk-sensitive control in economics, also known as KL-controlled, because this is a KL, or Bakliva, divergence.

It's a reducible control theory.

This is known as KL-controlled economics.

What it tells us is that this risk-sensitive control is basically what is left if we remove ambiguity.

But let's make the final move and actually take away ambiguity.

So now not only, sorry, let's make the final move and take away the risk, having taken away the ambiguity.

So by taking away the risk, what I'm saying is that I am equally uncertain what will happen if I do that in the future.

And if I take that risk,

with just this term here.

So I'll remove this now, and now I'm just left with this.

So what is this?

Well, it's just expected utility.

So this is what economists use to score the probability of choosing this policy or that policy.

In the absence of differential uncertainty, both in terms of ambiguity, but also in terms of risk.

There's a deep history to the expected utility

some reward or loss function that can ignore uncertainty, and then you will see policy of action selection being completely described by this value function.

So the purpose of that was really to illustrate how in general belief-based

the thing that we are trying to optimize, namely maximizing evidence, basing model evidence on models of the world, or minimize our uncertainty through active palpation of that world, are generalizations of things that we have all been working with for possible protection.

But you only get these special pieces if you remove uncertainty.

talk about value functions.

Just to make it very clear for those people who haven't come across information gain or epistemic value before, I just want to give you an intuitive example of what it means to reduce, to choose actions that dissolve uncertainty even before you know what's going to happen.

So imagine you're driving a car and you are looking around in the night time and you don't have a choice.

You're stopped at a traffic light and there is a filter on this traffic light and it could be pointing right or left.

And you can choose to either look over here or you can choose to look exactly at the sign.

Now, if you're wondering about driving, you're going to have a 50-50

belief, posterior belief or prior belief before looking over here, that the sign is pointing to the left or to the right.

And if you look over here, then you're not going to change that posterior belief.

So it doesn't matter whether the sign is pointing to the right or to the left, looking over here won't resolve any uncertainty.

You will have a 50-50 posterior belief

whether the sign is pointing this direction or in that direction.

So this is an example of a policy that has no epistemic value at all.

Contrast to that,

with a situation where you're looking directly at the sign, resolving ambiguity, and getting very precise sensory information.

So now, if the sign is pointing in this direction, then you will be 100% certain it's pointing in this direction.

If the sign is pointing in this direction, then you will be 100% certain it's pointing in that direction.

And you've known that.

And you've known that before you've even made an eye move.

fall into this salience, this information-gaining beings.

You can, if you know the way the world works, and you know and have a general knowledge of the consequences of your practice sounding out that world, you can work out in advance what the best move is to make to reduce your uncertainty.

And that's most of what I'm going to say from now on.

Rest

imperative and certainty-inducing self-evidence imperative.

So that's all the hard work done.

Now we're just going to see some pretty examples and simulations to show what it looks like in simulations and hopefully convince you that you've seen a lot of this phenomenology in papers and indeed possibly in your own research.

I should say

I made a joke about the mathematics and the formula before.

Perhaps I should excuse why we use the mathematics so much.

There's a simple reason.

If you really want to understand something, as Richard Feynman used to say, you have to be able to build one.

And to be able to build a little creature that does the paradigms that we ask ourselves,

animals to do.

You need to be able to write down the software and you need to be able to write down the mathematical formula on which the software are generated.

So that's our motivation.

It's really to create people in silica

as real creatures, and then see what this belief-updating, this act of infant self-emptying looks like, and then we can see the same kind of empirical phenomenology in real creatures.

So that's the excuse, or the motivation for it.

But it does require you to commit to very particular and well-specified

that are used by creatures, living systems, that explain their paradigm or their world.

And a very general one, again, please ignore the equations, we're going to walk you through the graphics on the model, is called a Markov decision process.

So with this single and same model, we have modeled an entire range of different kinds of behavior, ranging from

waiting games, foraging in a tea maze, sort of what bats would do, through to reading, we'll see an example of that later, through to active curiosity and problem solving.

So many, many different kinds of paradigms can be modelled with this one very general generative model.

So I'm just going to briefly take you through to give you one worked example, just to give you a feel, for instance,

to use these kinds of models to simulate your power plans.

So the idea is we need to generate outcomes, things that can be a creature or something like me would be able to observe, see, hear, feel.

And these outcomes, we're going to say, are going to be generated by hidden states of the world.

They are hidden because they're not directly observable.

We can only observe our sensations.

We can't see directly the causes of those sensations.

referred to as either latent or hidden states, and they have a narrative, they have dynamics, they have successions and transitions of hidden states.

So if I'm in a particular hidden state at this point in time, I will be probably sticking in this state at the next point in time, and so on and so forth.

So we have this cascade of hidden states, each state generating an outcome as time ticks along.

It's the probability of getting this outcome given this particular state of the world.

That's usually left by an A matrix.

Now clearly, the way that the world unfolds, these hidden states, depends upon how I act upon it.

It matters.

You know, the hidden state of

determines what they actually see.

So we imagine that some states of the world depend upon policies.

In other words, the transitions between this state of the world and the next state of the world at the next time point, encoded by a probability transition matrix B, is a function of the policy.

We've just said that policies are determined by our prior preferences and our interstates, and we're going to develop those prior preferences by C, remember that that will cost function, and they will have some precision, some confidence associated with that.

We've got a parameter called gamma.

I won't talk about that very much, but it is interesting because it

responses in the brain, and I'll show you a brief example of that later.

And finally, I just have to specify beliefs about the initial states, or components of the initial states, and a few hyperparameters that specifically parameterize it.

And with that model, I can model almost anything.

or anything furthermore if I make some simplifying assumptions about my beliefs about all the unknowns the key unknowns of course being the hidden states of the world generating outcomes and crucially the positives so these are the two things I need to infer for good beliefs about by minimising that by optimising that free energy

or that evidence you're about.

I'm also going to think about optimizing the confidence of my policies.

But the important thing here, notice that we're casting behavior here in terms of forming beliefs about what I could do, and then just selecting action from the most likely, or the policy that I think I'm most likely to be pursuing at the moment.

This is sometimes known as planning resilience.

So it's casting action as a process of inference.

So you actually, if you subscribe to this formulation,

choice on the selection of what to do next is an act of inference.

It's inferring this is the most likely thing that something like me would do in this belief state.

So I'm going to try and resolve that kind of uncertainty.

I'm going to work towards those sorts of preferences.

And you can write all of these particular characterizations down as prized in this kind of objective model.

If you...

then just apply something, this is called a greenfield approximation, just prioritize these beliefs with normal forms of those distributions.

You can then just go and get some shelf mathematics to describe the belief updating and the messaging.

From my perspective, even if you're not a mathematician, possibly from Milton's point, possibly not from your perspective, but from my perspective, the results are remarkably similar.

And more importantly, look very, very similar

to the dynamics and belief updaters you actually see in simplified versions of Brains.

So remember before I said there are just three things we don't know in this model.

State of the world, policies pi, and the precision or confidence placed with those policies.

And it turns out that the solutions that optimize that free energy function

can be expressed here as a non-linear function of linear exigence of beliefs about the past and the future

It's very much like a very simple neural network model.

A sigmoid firing activation function operating upon linear mixtures of activities elsewhere in the brain.

Very, very simple expression that now starts to provide a metaphor for neuronal responses

And it is also, this implies expectations about the past and the future.

So written into this genetic model is an elemental form of memory and prospection.

Postdiction and prediction, memory for the past and the future.

So there's a sense of time and progression implicit in this updating.

If we look at...

Interestingly, the confidence one over gamma here looks as though it's updated according to something that looks very similar to what prediction

dopamine and the relationship with reward prediction errors.

But I want to move on to optimizing the parameters of this model itself.

Again, it's very nice because the update rules, the solutions that optimize this free energy function are not exactly like they were in the early.

So you have these functions that have an associative term here.

Can't see this here because this is the D1, but if we were looking at the A1, you'd see the

indicator here and we just accumulate evidence by building connection strengths that then decay again as a function of time.

And then finally we have our action selection here.

And with these very simple rules you can start to engineer or propose a very crude or coarse functional anatomy.

They are used to update beliefs about states and the world, so they hit a campus at a time, and these beliefs are then used to evaluate the goodness of a policy in terms of the expected free energy, the risk and ambiguity, being coded in the front half of the brain, so basal, ganglia, corticothalamic loops.

confidence in his policies may be mediated by safe dopamine at the ventral tec-medical area.

And then later, right to the next action.

That changes the world, the world gives you the next observation, and so the cycle continues.

They're very crude, but relatively simple understanding of the computational anatomy that you get to.

So here's, I'll close now with two examples, one a very simple example of origin.

in their maids, and then we come to a brief survey of advances in deep generative models of this sort that have been used to understand things like language comprehension and reading.

So this is an example.

I don't need to go through it in detail.

In brief, what we have is a little rat or a mouse in a team mate, and it likes rewards.

What it also knows, though, is that there is an instructional cue at the bottom arm of the maze that tells it whether the reward is on the left or the right.

So that presents an interesting choice for this mouse.

It can go to one arm, and once it goes to one of the two rewarding arms, it has to stay there, which means that it can go there and get two rewards,

on 50% of the time, or it can go there and get nothing on 50% of the time.

Or, it can go down here to find out where the reward is, and then get, with 100% probability, the reward for half the time.

So the expected value is exactly the same.

But, by going to the instructional key, the epistemic key, it can immediately reduce the epistemic part of expected free entry.

which means that if this mouse was minimising its, was optimising its expected free energy, or minimising its ambiguity and risk, it should go, if we simulate those equations on the previous slide, with a generative model which I've written down here to be appropriate for this paradigm, it should go and get the extent Q, and then go and get its reward.

And indeed, that's what it does.

So it starts off, what I'm showing here, is behavior over 32 trials in terms of where the reward was, whether it was on the left or the right, and the policy that it chose, the outcome, the amount of reward it got, and beliefs about whether the reward is on the right or the left in terms of the additional space.

is after switching the reward up to the first couple of presentations, I then left the reward on the left-hand side.

And I want to see what's going to happen.

So initially, as we might anticipate, the mouse goes and finds the epistemic view, it's always this uncertainty about what to do, and then indulges in its risk-averse behaviour, so then chooses the pragmatic approach.

a preferred option by going straight to get the reward, and it's as happy as it could be.

However, as time goes on, it now learns that, in fact, the reward is over there all the time.

So now, the epistemic value of that instructional cue gets less and less and less, which results in less and less uncertainty because it's increasingly certain that the reward is on the left-hand side due to

So at one point, it changes its preferred policy and jumps to a pragmatic exploitative policy.

So we've got this natural progression from exploration to exploitation that is purely a reflection of the

goodness, or the thing to choose, depends upon my beliefs and my uncertainty, whether I need to write that certainty or as a search.

It depends upon the need for search.

And of course, they're very familiar in that environment, there is no need to search.

And to be a strength for that expected value and engage in exploitative behavior.

This slide just summarises that and makes exactly the same point that I've been making.

So, basically learning underwrites confident policy selection and that confidence is reflected in this precision parameter which we spend a lot of time on.

I don't mean that.

Indeed, when we stimulate

It looks almost exactly like dopamine.

We can also look at simulations of ATP updates during a particular trial, so it makes a move, it sees this, makes another move, because as soon as it sees anything, it has to iterate these equations in order to

the Bayes optimal solution, the self-evidency solution, and that looks a lot like an event related potential in electrophysiological research.

And interestingly what happens is it was less belief updating when it's more familiar and confident about the environment and you get an attenuation of these event responses, but an increase in the confidence because it knows exactly what's going to happen,

and indeed what it expects to happen does indeed happen, and it happily goes and exploits its knowledge about where the reward is.

So using that different example, you can tell all sorts of stories.

You can tell a story about the representation of the future and the past, so this is the beginning of the trial, the first one, the second move, and of course, as these beliefs

what our beliefs about the future consequences of action now become memories of our past.

So there's an interesting shift of time of temporal frames of reference that means that things that were once predictions have become post-predictions.

That becomes very interesting when you accumulate beliefs from trial to trial.

It also allows you to think about

to the things that you will ultimately choose as opposed to things that you are not going

we can even plot these responses as a function of where the mouse is.

And what emerges from this kind of architecture are place hints, place notes, that sometimes are very unambiguous.

For example, the two rewarding locations sometimes are a bit more ambiguous.

We can also perform simulated

So, these are the same results that I showed you before, but I'm now telling a different story about them using a different language, as if I were an electric physiologist doing bottle paradigms.

So what I can do is look at the belief updating, where this will now seize the same stimulus when it's familiar with it, and when it's not familiar with it.

The only thing that's different between the perception and the action is its beliefs that it has accumulated through experience.

And if I associate this with a standard stimulus and this with a novel or an old-fashioned stimulus, we can compare the belief up-to-date to entail the difference, and indeed we can reproduce the phenomenology of this much negativity.

Do the same thing with those stimulated dopamine responses.

in single-unit electrophysiology in dopamine cells, namely a transfer of phasing responses from the rewarding cue, the unconditioned stimulus, to the instruction-like systemic cue, which you can think of here as the conditioned stimulus.

So again,

Nothing's changed, really, other than I've told a slightly different story about the results that emerged from this little simulated rat.

And all of them lend a degree of constipability to this overall phenomenon.

maximizing evidence to the model of the world and selecting actions that minimize uncertainty, namely this .

So I'm going to finish now with a very quick run through of exactly the same technology and ideas that apply to slightly more sophisticated

It's educational.

We spent a long time drawing it.

Again, that was a joke.

It's a nice graphic because once you've written down formally what you think is driving message passing, belief updating, and behavior,

you can now just extend that formalism by generalizing it to hierarchical structures.

And when you do that, you start to see lots of emerging behaviors that now look a lot more

is taking our standard little MDP model, space kicking over, time one, time two, generating through the likelihood matrix A, and I'll come here, and the transitions are carried by the B matrix, depend upon some policies pi, that are informed by the expected free energy G, the goodness of those policies.

What we've done is put another one of these on top,

it operates at a slower time scale.

So the outcomes from the process at the higher hierarchical level now cause things that don't change on a faster time scale.

There are lots of things we could have chosen.

We could have chosen the likelihood of major seats, or we could have chosen the likelihood of a particular policy.

We've actually chosen here just the initial state.

but it means that the outcomes from the generative model's point of view of the higher level are in play for the duration of the state transitions at the lower level.

And you can imagine putting a faster level in this, and a faster and a faster one.

So you're writing in, you're baking in to your generative model, not only a high mark of depth or abstraction,

but also a deep diachronic on time depth, a separation of temporal scales over time.

And of course that's what we need to understand language.

I will have a representation on a sentence or a phrase at one level, and that's the same sentence or phrase from the beginning

It's the same object.

But at a faster time scale, this current word will change.

But this current word is the same word from the beginning of the words, first time frequency glide, phoneme possibly, to the last one.

And as we keep going lower and lower and lower, we now generate faster and faster dynamics using this kind of model.

So you may be asking,

of the first memory used in the rat into this deep diachronic structure.

This is exactly the same model, and the reason I show this, and the reason I like this model, is you can generate this graph automatically from this graph, and this graph is known as a factor graph.

Now, it may not mean very much to psychologists, but if you're a computer scientist,

and you want to design the message passing in the most efficient way, this is the design.

So, what we're saying here are what this figure says.

If you can write down the form of your generative model, you have automatically written down the message passing graph, and at some level, a brain has to be used in terms of connections and patterns

They place the variables on the edges and the probability distributions at the nodes.

That's why they're called factor graphs, so the probability distributions are the factors of identity of what are the margins.

You can forget that if it wasn't interesting.

What is interesting to remember is you can generate these things automatically, and once you've done that, you can start to make little brains in software.

Anyway, so once you've written down that factor graph and you've learned the architecture of the message passing, you can actually go to the Bureau of Anatomy and ask, where are the isomorphisms in terms of the structure of the dynamics, the temporal scheduling of those messages in real brains?

And it's an interesting jigsaw problem to solve, but there are lots of immediate

here.

We don't need to talk about this.

The point is that there's a very interesting opportunity.

Once you understand what has to be the computational anatomy, if you commit to this generative model approach and self-evidency formulation of belief-updating behavior, if you commit to that, then you've got a necessary

computation anatomy, you've got the empirical neural anatomy that we presume does this computation, so now we can start to look for parallels and assign different roles to different parts of the brain.

So for example,

in reference to that paragraph from the previous stage.

But let me finish now just by taking you through conceptually a generative model of a simulated agent is doing a very simple form of pictographic reading.

So here there are no letters but there are little icons and the position of these icons prevents a particular word.

An icon comprises, each word, if you like, comprises two icons.

There could be seeds, a bird, or a cat.

And if the cat is next to the bird, that means the bird will flee, so that's the word flee.

If the seeds are next to a bird, that means the word feed.

The bird can feed on the seeds.

If, however, there's nothing next to the bird, so the seeds are down here in the diagonal corner, that just means wait.

So it's a very simple little language that we've arbitrarily

And the reason for sort of using this pictographic form is that the agent has to decide where to look.

If he wants to read this word, look at the letters in the word, it has to decide where to look at that.

It can look over here, over here, over here, over here, as denoted by the locations one, two, three, and four.

From the point of view of the generative model, what does that mean?

These are the states, the hidden states that you would need to generate an outcome.

So what would be the sensory outcome?

Well, the sensory outcome would be feeling that I'm looking at positions one, two, three, or four, or what I'm actually sampling or foveating at that time, which can either be nothing, seeds, a bird,

or a cat.

But to generate those outcomes, I have to know the configuration of these pictographic letters, where I am looking, and I've introduced another hidden state here, which is flitting, like presenting words in upper or lower case.

So with those three causes, I can generate any particular outcome in a visual modality,

and a proprioceptive, or feeling where I am currently pointing my eyes, modality.

And because I have written down the generative model, I can use that standard message passing scheme of the previous slide to simulate inference.

I can simulate what this little creature would do in terms of foraging through

I want it to actually remember the words it's seen, and from the point of view of the gender model, generate sentences or sequences of words from the point of view of self-evidencing, inverting that gender model to recognize what this word is in the context of beliefs about what the sentence is.

So to do that, I now have to put together four words on four different pages, if you like, and

at a higher level, which we actually did, but I'm not going to show here.

So now if I know the sentence, I know the sequence, one, you know, feet, weight, feet, weight, and I know where we are in terms of which page we're looking at, I can now generate the word, I can generate the word, if I am now reading the information about where I am looking and whether I flipped or not, I can now generate the outcome.

And if I can generate the outcome, that means I can invert the outcomes

So we're going to take that factor graph, that computational anatomy, and this generative model, and then simulate reading in terms of where this agent looks to try and accumulate evidence and build posterior beliefs about the sentence it is reading.

And the actual sentence it is reading is fleet weight, feet weight, and these are the

there at the lowest level.

And the key point made in these simulations is you can have very precise beliefs about what you would see if you looked over there, even though you never actually looked there.

And these precise beliefs come from this deep structure.

So for example, you can see that in the first room,

the second word.

At no point does it actually sample a stimulus that it either sees or burns.

It sees nothing on either sample and yet it knows because it knows what could possibly happen in terms of the alternatives and the structure of the sentence that the solution has to be

Seas up here, and birds up here, and indeed its posterior beliefs is hallucinating effectively in a very positive and base-optical way.

The existence of these percepts, even though we never looked there, and you see evidence of that if you look in detail, the sequence of saccadic eye movement

they resolve uncertainty, they respond to any systemic affordance, and go and get the next sort of information that will resolve uncertainty about what this agent is looking at.

I've shown the same results here, but in a different format, just to emphasize the separation of technical skills.

So, these are beliefs at the highest level,

word.

So he knows almost immediately he must be looking at one of these two sentences because the first word is unique to these two sentences, but the last word disambiguates.

So this uncertainty evolves slowly and it maintains slowly and it's only resolved at the last word.

In contrast, the beliefs about what particular word I am currently looking at develop much, much more quickly.

belief, and then that is even evidence for the higher sentence-based belief, and then we start again with a new outcome, a new outcome, a new outcome, and it's hard to select visual impressions.

And I've tried to indicate that in terms of this separation of temporal timescales dictated only by the belief updating.

Just want to make a point again that what you actually see in these

in silico creatures is very similar to what we see as an electrophysiologist.

So, for example, presacadidomacrid activity in the prefrontal cortex, shown here in a raster format, looks very similar to the kind of responses observed in some of the new recordings shown here in terms of a bar chart.

Furthermore, when you look at the deflections associated with the belief updating on a stimulus-by-stimulus basis, we see something that looks very much like the pericicadic evoked potentials during anti-vision in monkeys.

And I just want to close by pursuing that, by focusing on the responses to stimuli towards the end of the sentence.

And I'm going to play a trick on this little creature.

I'm going to introduce a violation

or two violations of a different sort, in the hope of reproducing canonical responses you find in cognitive neuroscience, namely a pre-attempted like this much negativity at about 100 to 170 milliseconds, and then a reorienting novelty-like response at later

mean to the word.

So I'm going to do that just by presenting it in other case.

I'm going to present it in low case.

So this is, if you like, the steepest manipulation, the low level.

And I've shown the results of the low level in blue without the manipulation.

And then the difference here, it looks very much like a mismatch in negativity.

on the second level, all the more semantic representations.

If I now do the same trick, but this time, instead of just changing uppercase to lowercase, I now have to change the meaning of the word, but use the same stimuli, I now have got a much more semantic high-level violation.

And now the thing that recognises the surprise, or the free energy, here,

second semantic level.

But of course, it's had to wait longer to get the evidence from the first level to be surprised, to do with the belief update, and to respond to that surprise, which means that the difference waveforms are now expressed in peristinguish time in a regime that would correspond to the P300.

So it's a very particular, specific example.

But I use this just because of how far you can get in understanding

of the computational architecture and message passing under the simple imperative to minimise uncertainty about the way the world works and indeed how can I work in that world.

And this conclusion I think is the nicest summary

many of the inferential interpretations of the skin.

So each movement we make by which we alter the appearance of objects should be thought of as an experiment designed to test whether we have understood correctly the invariant relations of the phenomena before us, that is, their existence in definite spatial relations.

And with that, it remains for me to thank the people whose ideas I've been talking about, and most of all, thank you for your


SPEAKER_01:
into the biological reality.

So to clarify something, can we return to your first example with the old?

And so I have two questions related to this first example.

The first one is how in terms of...

How can you describe in your theory the hungriness of the owl?

Why is the owl hungry?

And why the bird has a belief that the prey should be cached?

It's the first question.

And another one is related, and maybe it's how I'm trying to resolve this problem.

The question is, how is active inference related to natural selection?

And because testing hypothesis is risky, it's time and energy consuming.

I mean that you are resolving uncertainty, you invest something in terms of your energy and your time and so on.

And what do you think about how natural selection shapes these prior beliefs to make them

more efficient in terms of survival, not only in terms of complete .


SPEAKER_02:
So two excellent questions.

Probably three questions there.

So the first question, how do I account for context

of behavior for me when I'm very hungry, as opposed to when I'm just eating.

So in this scheme, all of the answers usually reduce to the form of the generative model.

So here, that would be a nice example of having a hierarchical generative model, where your

different states of people.

So if I were to, and now we're interested in that, we are entering the interesting world of active influence applied to homo-spaces, balanced spaces, and interoception.

And let's say I have evolved, pre-empting the next question, to have a generative model and a brain.

There was a

good explanations for particular gut feelings and particular particular inputs from say receptors measuring blood sugar and also my beliefs about when I last had something to eat then if I had those representations I would be able to do two things first of all

I will be able to contextualize my client preferences about what I think I will be doing in half an hour.

Which could be being in a restaurant, or it could be continuing to work, or it could be continuing to socialize.

So you can contextualize any behavior simply by conditioning a parameter of

if you have a sufficiently deep projector on.

And that leads to all sorts of interesting issues, you know, the distinction between homeless spaces and other spaces.

So I would imagine that a virus, unlike the animal, doesn't worry about whether it's hungry or not.

It will just want, let us take an E. coli.

The E. coli doesn't worry about whether it's hungry or not.

It just pursues.

I'm not sure.

It's relatively skeptical, isn't it?

Let's assume a thermostat doesn't care whether it's hot or warm.

So it just immediately minimizes.

It chooses policies of a very trivial sort, which is just the very next

that then change to the homeostatic set point.

That's very different from having another level above, which recognizes a different state that would then actually change the set point.

At that point, we move from a homeostatic reflex to an outer space.

It's where we now start to anticipate the consequences of our behavior for our hyperspaces.

So I will go and eat something before I need to have a glucocon autonomic reflex to feel like very, very low amounts.

So I think it's a great question because what it speaks to is the fact that we're not dealing with simple general models of the sort appropriate for a thermostat or a virus.

We're dealing with very deeply structured, and I repeat, diatronic in the sense of separation of temporal time scales, which are the kinds of models which would be necessary

And then the question is, well, where did those learn from?

And then I could answer that by answering your third question,

How many levels does my model have?

How many hidden states?

So if you were doing machine learning and deep learning, say using a variational encoder, you have prior beliefs when you say, yes, there are 12 hidden layers, and hidden layer six has 270 hidden units.

All of these are form priors that you believe are fitting the purpose and appropriate to the kind of data that you want to classify.

prior expectations, aspersions, and anything that you would normally associate with a prior belief.

So, by model, I mean promise.

Obviously, there is a likelihood, but that's usually quite true, because the likelihood just lives at the bottom of these models.

So, if we go to this graphic here, all of these are promise.

The only time the likelihood

becomes evident, it's right at the bottom here.

So all the interesting structure in terms of its depth and conditioning and contextualization is in the prior structure, and the actual quantitative or parametric priors you apply to all of these variables.

So that brings me to, or you, to your second question, where do they come from?

And again, you appeal to hierarchical

difference, but now over a more extended timeframe.

So from the point of view of statistician, the way that you would learn the good priors for a particular environment or a particular set of data via manifold learning, for example, or structural learning, which is mainly a model selection.

So now

and maximizes modern evidence at an average during time.

And if you look for the physiological hominid thoughts of that, it could be things like life or this or see.

I will be giving a lecture that illustrates that point

But as you've hinted, all of this takes place at an evolution time scale.

So the way that mathematically you would bring all of these things down under the same principle

or dynamics or the replicator equation or price equation.

These are very easily shown with a few simplified assumptions.

They can be shown to be Bayesian filters or Kalman filters.

So what you're saying is that

Evolution is just nature's way of doing basic model selection, to change or to select those models, those prior structures, those phenotypes that have the greatest evidence.

What do you mean by evidence?

The likelihood that they are there.

The likelihood that they are an outlaw for that environment.

So from the environment's point of view, it is testamentary

evidence that it is a good fit for you, the environment, corresponds to adaptive fitness, that is scored by the time interval of the variation of the energy because that's the downward of the probability of that model being the right model given the experience provided by the environment.

In that context, you wouldn't think about phenotype as being the individual, you'd think about conspecifics, the cycle of life, over a developmental cycle.

Does that make sense?

Thank you very much for your talk.

I have a small question about, is there a smart body?


UNKNOWN:
about human evolution.


SPEAKER_05:
Is it possible, in terms of your theory, to describe, just to describe and explain, the huge quality difference between humans and other animals?

And is there, maybe in that presentation part of the theory, any hint that you think will help solve this problem?


SPEAKER_02:
Thank you.

Yes, that's a good question.

I think it must be a short answer because I think it speaks very much to what we were just talking about.

I think it's just the depth of the gender model that distinguishes the kinds of structures that you associate with being human.

the organism, the multicellular organism, it immediately speaks of a hierarchical aspect, a nesting aspect.

But if you take that further, I just think, what kinds of hierarchical extensions would make you you as opposed to a bacterium, or me as opposed to a bacterium?

I think the answer to

I think it almost reduces to the very simple observation that you and I plan, and the bacterium doesn't.

We can argue about whether the bee does or the dog does, but certainly the bacterium, during its chemotaxis, does not plan.

It does not plan to go to school or to buy birthday presents, whereas we do.

So what does it need to plan?

To plan is to select.

which means you have to have a generative model of the future, the consequences of those concepts.

So that's just simply a statement of the fact that we infer our world under generative models that have a temporal depth, that have a horizon that goes beyond the present, that enables us to plan and to think about what would happen if I did that.

Now, because as soon as you have

to make a choice.

So the bacteria doesn't have a choice about what it does, but you and I do.

And I think that sort of qualitative distinction, which again is just a structural fire, is

It's not to say that we're better than the bacteria or the virus.

For any particular eco-environment, there will be a free energy extreme.

So viruses are great for fitting and inferring their

meaning their environment, which is usually another person's cell, another cell, we'd be very bad at that and have to compress you down very, very small.

You would not do very well inside a cell and the virus would not do very well in the university.

So it's all in relation to the way you model your sensory inputs from your environment at your time scale.

And once you acknowledge that, then there are lots of

global minima or global maxima, that correspond to all sorts of different ways of being, both in terms of species, but also different phenotypes within the species, then you can see now an easy taxonomy in terms of hierarchical depth and sophistication, and particularly temporal depth.


SPEAKER_05:
Thank you very much for the lecture again.

As I said at the very beginning, I would like to put it a little bit away from, let me say, scientific point of view towards the theoretical future prediction.

Do you think it is possible, first question, to fit current state of psychological outlooks towards personal belief motivations and beliefs into the scheme that you're describing?

beliefs that are wrongly put into the person, and they could also be hidden states of the person, which is not recognizing it.

And if so, is there a possibility, from your point of view, to create an AEM-impact software which would be possible to predict behavior

each kind of like certain person in certain conditions, by that given kind of enormous power outside, you know, to manipulate those people.


SPEAKER_02:
Thank you.

Right.

So a fascinating question.

I just want a bit more aspect to focus on.

Perhaps I'll just take a practical one and take your

a good thing or a bad thing, but certainly being able to do it and understand what you're doing is practically very relevant in a therapeutic conference.

It could be psycholimiting or it could be biogenified rights.

a conviction in some state of affairs, usually interpersonal, for which there is no evidence that a normal person would accept as evidence for that condition.

You could even take this to things like anorexia and dysmorphophobia, holding beliefs about myself

I see myself as fat.

I have seen, I have heard that I am fat even when you might see me as being very thin.

So you can certainly get an enormous variety of posterior beliefs.

I would say that like a constable, which you bring down to...


SPEAKER_05:
a certain number of fears, like death, hunger, whatever, just having, that's why I'm asking, so those, I believe, are not, um, incontestable to me, they're, can be brought to, like, maybe big, but still, final number of, uh,


SPEAKER_02:
I think there will be mathematical reasons based on complexity to make that true.

I guess what I really meant was that the number of ways in which I can deviate from you are large, but the number of dimensions may be quite limited.

So if that's the case, if there is an opportunity, and indeed there's clinical evidence for a better belief updating,

then people have been focusing on how that could happen.

It seems that the most important synaptic mechanism that enables that sort of false inference is in the precision afforded to various sources of sensory evidence.

you'd think of that in terms of attention.

So now the game becomes shifting from how we form our beliefs or update our beliefs to how we attend to different things.

And we were talking just before about this being the same mathematical problem as knowing what is fake news versus non-fake news in the millennial society and

create or assign a precision that enables that sort of information to update your beliefs.

So I think in terms of understanding the mechanics of belief updating and intervening in that, either therapeutically or for commercial reasons, like in advertising, for example, it's probably going to be all about how you indicate attention and what you mean by attention and the physiological mechanisms of attention

some high-level control over attention, to be able to mentalize it.

So there are certain forms of attention which we have known, not because I don't have any control over, which it may be possible to learn to control with suitable mindfulness training, for example.

So, saccadic suppression.

So this is the phenomenon when I move my eyes as I look around the room.

are the static samples that I then integrate into a coherent theme.

So that saccadic suppression is a very interesting example of temporarily ignoring or reducing the precision of the light level mapping here in a very, very terribly precise way.

by beliefs higher in the model.

Now imagine that you now had a connection in the generative model.

Say you felt hungry, so we talked about contextualising before.

Say there was some higher level representation of self, or myself, state of self, at this point in time, you could actually gain

and then suspend.

That's the kind of suppression.

So you can choose whether to see the world moving when you move your eyes or not.

In fact, to a certain extent you can.

This is a Neil Handel string.

You can see what it would look like if you just push the side of your eyeball.

And you see the world shift.

So there are ways of getting around it if you have a sufficiently deep generative model.

So my question, I'm not sure what the implications were in terms of the ethics of it or the weaponization of that, which is another issue, of course.

Just in terms of practically what you would be looking at, I think you'd be looking at the processes, the media, the precision, and the degree of message passing between these hierarchical levels that would look very much like what you treat as newsworthy and what you ignore.

If you can get control of that for yourself and possibly for a patient, then you may well have an enormous therapy team.


SPEAKER_05:
Thank you.

I wasn't talking exactly about the theory.

You can give me a copy if you'd like.

Just... Well, it is the same thing, like, as you said, advertisement rates.

It's not, like, kind of changing human attitudes, explaining how fast, how bad, or whatever, you know, advertisement.

Disorder will react to this kind of, like, what will be the way that people will be based on that kind of disorder, non-psychiatric, like epithelial level, you know, cut it down, to this certain message, like the non-hedronized types of that?

So will they still buy?

Will they still like, will they say no?

Will they say yes?

I'm talking about kind of like prognostication model, not like improvement.

But just improvement of the management, not improvement of the agents themselves.

So all this in the economics context then?

Socially, politically, whatever.

Like as you say, monetizing or like whatever.


SPEAKER_02:
I don't know because this particular mathematical formulation

And there have been sort of multi-agent simulations at the level of cells organising to form patterns and coalitions.

But nobody to my knowledge has taken these variational free energy solutions to model markets or advertising or geopolitical events.

But it's a really exciting opportunity.

Thank you very much for an interesting presentation.


SPEAKER_00:
I would like to ask several questions concerning Markov processes.

First of all,

of market processes is that the future is determined by the past only via the present.

If this factor resulted from the application area or it's only the simplification to be able to solve mathematical problems associated with problems to solve.

The second question is, on the slide devoted to activity and occurrence, there is two magnitudes, entropy and energy.

Entropy is non-dimensional, energy is dimensional.

How do you explain the difference between them?

And the third question, do we really solve ordinary differential equations to predict the career of Markov worlds?

Thank you very much.


SPEAKER_02:
Right, thank you.

So, three easy questions, right?

Does anybody want to answer in my department?

We probably have a new physicist.

So, can I answer as a physicist?

So a really interesting question, the first one.

To what extent are Markovian independences or Markovian

for convenience, or do we think it is one thing that these are the only kinds of generative models that are fit for the universe in which we live?

I think technically all of this maths in

the Markovian properties would be true and fundamental, and that's where you start.

So my answer would be they are not a device of mathematical convenience.

They've actually baked in to the underlying premise.

And that also enables me to disagree.

they could be applied to the density dynamics for example a Fokker-Pack equation on the density dynamics or a Scrooge-Waig equation or a master equation so much of this really all of these simulations are actually upgraded to sense on functionals where there's a functional probability distribution very much like the Fokker-Pack equation so that's the belief based aspect that we're moving away from creating

in terms of gradient flows that are functional on beliefs and coding-like states.

It's a big technical review.

If you'd like to discuss that further in email, I'll send you something which I've never dared submit for peer review that takes you through those arguments very precisely.

The interesting thing, though,

the first question, because clearly this particular

MDP is semi-Markovian.

I have broken the Markovian property because I've now got two timescales in place here.

So in practice, what happens with these immensely complicated itinerant captain sets that we use as a mathematical model of self-organizing systems, like biological systems, when you summarize it in terms of a Markovian

like it has now acquired a semi-Archovian aspect.

So it is interesting that from the purely Archovian physics of it, due to the complicated nature of the systems that we like to study, when you find the free energy optimising

and breaking the simple Markovian to make them into the same Markovian.

So, at that point, I think the discreet model of decision process does become a mathematical approximation to what is actually a continuous time Markovian process that looks as if it's lost its simple Markovian property because it's just become so itinerant, so complicated, so structured, and so interesting from a biological perspective.